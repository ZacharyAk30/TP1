{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP MLFLOW\n",
    "Florent Jakubowski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Prise en main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Installez le package mlflow avec python dans un environnement virtuel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Dans un terminal lancez un serveur mlflow. Aller voir dans votre navigateur, sur le port correspondant, l'ui de mlflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mlflow server --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow server --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque vous lancez mflow sans option par défaut mlflow va stocker toute la donnée dont il a besoin sur votre file system. Vous aurez notamment un dossier mlruns qui se créra par défaut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Dans un notebook, utilisez le package mlflow pour vous connecter au serveur mlflow que vous avez lancé. Utilisez la bonne fonction pour paramétrer l'adresse du serveur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention ! A chaque fois que vous effectuerez une opération sur mlflow dans une autre cellule de votre notebook vous devrez vérifier avant que vous pointez bien sur le bon serveur mlflow. Il existe aussi une fonction pour connaître quelle adresse de serveur a été enregistrée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://localhost:5000'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créez une experiment via votre notebook ou avec via l'ui. \n",
    "Une experiment, ou une expérience en français, est un ensemble de run que vous avez effectué. Le but est de trouver in fine les meilleurs paramètres, modèles ou hyperparamètres pour votre besoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment already exists\n"
     ]
    }
   ],
   "source": [
    "try : \n",
    "    experiment_id = mlflow.create_experiment(\"My Experiment\")\n",
    "    print(\"Experiment created with ID: \", experiment_id)\n",
    "except:\n",
    "    print(\"Experiment already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pareillement à l'adresse du serveur mlflow à chaque fois que vous exécuterez un run dans une cellule vous devrez définir l'experiment sur laquelle vous voulez envoyer votre run. Cherchez dans la documentation la fonction permettant de faire cela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/636497380083101659', creation_time=1700040769157, experiment_id='636497380083101659', last_update_time=1700040769157, lifecycle_stage='active', name='My Experiment', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"My Experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous vous proposons de créer une fonction `configure_experiment` permettant de créer une expérience ou de définir l'expérience si elle existe déjà."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_exp(name):\n",
    "    # Check if the experiment exists\n",
    "    experiment = mlflow.get_experiment_by_name(name)\n",
    "    \n",
    "    if experiment is None:\n",
    "        # If the experiment does not exist, create it\n",
    "        experiment_id = mlflow.create_experiment(name)\n",
    "        print(f\"Experiment created. ID: {experiment_id}\")\n",
    "    else:\n",
    "        # If the experiment exists, set it\n",
    "        mlflow.set_experiment(name)\n",
    "        print(f\"Experiment set to {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Après avoir créé l'experiment. Nous allons entraîner notre modèle. Prenez le dataset wine de la librairie sklearn et utilisez un algorithme de la famille des arbres de décisions. Nous allons lors de l'entraînement de notre modèle logger les mesures.   \n",
    "\n",
    "Pour cela nous voulons lancer un nouveau run dans notre experiment sur mlflow. Le run correspond à un entraînement du modèle.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Créer une variable run_name pour le nouveau run que vous voulez créer avec la date du jour, l'heure, la minute et la seconde dans le nom.   \n",
    "-Trouvez comment logger les paramètres d'entraînement, les hyperparamètres et les performances du modèle (metrics) explicitement dans mlflow en lançant votre run de manière manuelle.   \n",
    "-Ajoutez également votre modèle avec un nom distinctif grâce à la méthode adéquat, vous devrez réutiliser ce nom lors des prochains entraînements. Que voyez-vous dans l'ui de mlflow ?   \n",
    "\n",
    "\n",
    "Vous pouvez utiliser `with` pour ne pas avoir besoin d'utiliser la fonction `end_run()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained and logged with run name:  20231115-104228\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the wine dataset\n",
    "wine = datasets.load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a decision tree classifier\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Train the classifier\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set results\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Create a run name with the current date and time\n",
    "run_name = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Start a new run\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "    # Log the model parameters and metrics\n",
    "    mlflow.log_param(\"criterion\", clf.criterion)\n",
    "    mlflow.log_param(\"splitter\", clf.splitter)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    # Save the model with a distinctive name\n",
    "    mlflow.sklearn.log_model(clf, \"model\")\n",
    "\n",
    "print(\"Model trained and logged with run name: \", run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut utiliser `mlflow.active_run()` pour être sûr que le run est bien terminé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.active_run() is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ecrivez un run sans utiliser un with et sans utilisez mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run started with run_id:  29817e4e842246de950e02f785397f8d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "run = mlflow.start_run(run_name=\"My Run\")\n",
    "mlflow.log_param(\"param1\", \"value1\")\n",
    "mlflow.log_metric(\"metric1\", 1.23)\n",
    "mlflow.sklearn.log_model(clf, \"model\")\n",
    "\n",
    "print(\"Run started with run_id: \", run.info.run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `active_run()` nous retourne normalement ce run ci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ActiveRun: >\n"
     ]
    }
   ],
   "source": [
    "run = mlflow.active_run()\n",
    "print(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faisons bien attention de bien le fermer pour ne pas avoir de comportements exotiques ensuite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisez mlflow.end_run() et vérifiez avec active_run() qu'aucun run n'est retourné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.end_run()\n",
    "mlflow.active_run() is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe une autre manière de logger automatiquement des paramètres et des metrics à vous de la trouver.   \n",
    "(Attention une fois activée cette fonction entrainera toujours un log automatique, veillez à la désactiver pour la suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/15 10:48:10 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\"\n",
      "2023/11/15 10:48:10 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\"\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "C:\\Users\\zacha\\ApplicationDev\\miniconda3\\Lib\\distutils\\core.py",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\Mlflow_tp_efrei_etudiant.ipynb Cell 31\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zacha/Documents/CourS9/ADDE92Applications%20of%20Big%20Data/TP1/Mlflow_tp_efrei_etudiant.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msklearn\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zacha/Documents/CourS9/ADDE92Applications%20of%20Big%20Data/TP1/Mlflow_tp_efrei_etudiant.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m mlflow\u001b[39m.\u001b[39mautolog(log_models\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, exclusive\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zacha/Documents/CourS9/ADDE92Applications%20of%20Big%20Data/TP1/Mlflow_tp_efrei_etudiant.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m mlflow\u001b[39m.\u001b[39;49msklearn\u001b[39m.\u001b[39;49mautolog(log_models\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zacha/Documents/CourS9/ADDE92Applications%20of%20Big%20Data/TP1/Mlflow_tp_efrei_etudiant.ipynb#X41sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m clf \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\mlflow\\utils\\autologging_utils\\__init__.py:424\u001b[0m, in \u001b[0;36mautologging_integration.<locals>.wrapper.<locals>.autolog\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[39mwith\u001b[39;00m set_mlflow_events_and_warnings_behavior_globally(\n\u001b[0;32m    406\u001b[0m     \u001b[39m# MLflow warnings emitted during autologging setup / enablement are likely\u001b[39;00m\n\u001b[0;32m    407\u001b[0m     \u001b[39m# actionable and relevant to the user, so they should be emitted as normal\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    420\u001b[0m     disable_warnings\u001b[39m=\u001b[39mis_silent_mode,\n\u001b[0;32m    421\u001b[0m ):\n\u001b[0;32m    422\u001b[0m     _check_and_log_warning_for_unsupported_package_versions(name)\n\u001b[1;32m--> 424\u001b[0m     \u001b[39mreturn\u001b[39;00m _autolog(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\mlflow\\sklearn\\__init__.py:1261\u001b[0m, in \u001b[0;36mautolog\u001b[1;34m(log_input_examples, log_model_signatures, log_models, log_datasets, disable, exclusive, disable_for_unsupported_versions, silent, max_tuning_runs, log_post_training_metrics, serialization_format, registered_model_name, pos_label, extra_tags)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[39m@autologging_integration\u001b[39m(FLAVOR_NAME)\n\u001b[0;32m    977\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mautolog\u001b[39m(\n\u001b[0;32m    978\u001b[0m     log_input_examples\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    991\u001b[0m     extra_tags\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    992\u001b[0m ):  \u001b[39m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    994\u001b[0m \u001b[39m    Enables (or disables) and configures autologging for scikit-learn estimators.\u001b[39;00m\n\u001b[0;32m    995\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1259\u001b[0m \u001b[39m    :param extra_tags: A dictionary of extra tags to set on each managed run created by autologging.\u001b[39;00m\n\u001b[0;32m   1260\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1261\u001b[0m     _autolog(\n\u001b[0;32m   1262\u001b[0m         flavor_name\u001b[39m=\u001b[39;49mFLAVOR_NAME,\n\u001b[0;32m   1263\u001b[0m         log_input_examples\u001b[39m=\u001b[39;49mlog_input_examples,\n\u001b[0;32m   1264\u001b[0m         log_model_signatures\u001b[39m=\u001b[39;49mlog_model_signatures,\n\u001b[0;32m   1265\u001b[0m         log_models\u001b[39m=\u001b[39;49mlog_models,\n\u001b[0;32m   1266\u001b[0m         log_datasets\u001b[39m=\u001b[39;49mlog_datasets,\n\u001b[0;32m   1267\u001b[0m         disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[0;32m   1268\u001b[0m         exclusive\u001b[39m=\u001b[39;49mexclusive,\n\u001b[0;32m   1269\u001b[0m         disable_for_unsupported_versions\u001b[39m=\u001b[39;49mdisable_for_unsupported_versions,\n\u001b[0;32m   1270\u001b[0m         silent\u001b[39m=\u001b[39;49msilent,\n\u001b[0;32m   1271\u001b[0m         max_tuning_runs\u001b[39m=\u001b[39;49mmax_tuning_runs,\n\u001b[0;32m   1272\u001b[0m         log_post_training_metrics\u001b[39m=\u001b[39;49mlog_post_training_metrics,\n\u001b[0;32m   1273\u001b[0m         serialization_format\u001b[39m=\u001b[39;49mserialization_format,\n\u001b[0;32m   1274\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[0;32m   1275\u001b[0m         extra_tags\u001b[39m=\u001b[39;49mextra_tags,\n\u001b[0;32m   1276\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\mlflow\\sklearn\\__init__.py:1822\u001b[0m, in \u001b[0;36m_autolog\u001b[1;34m(flavor_name, log_input_examples, log_model_signatures, log_models, log_datasets, disable, exclusive, disable_for_unsupported_versions, silent, max_tuning_runs, log_post_training_metrics, serialization_format, pos_label, extra_tags)\u001b[0m\n\u001b[0;32m   1820\u001b[0m     allow_children_patch \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1821\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1822\u001b[0m     estimators_to_patch \u001b[39m=\u001b[39m _gen_estimators_to_patch()\n\u001b[0;32m   1823\u001b[0m     patched_fit_impl \u001b[39m=\u001b[39m fit_mlflow\n\u001b[0;32m   1824\u001b[0m     allow_children_patch \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\mlflow\\sklearn\\__init__.py:98\u001b[0m, in \u001b[0;36m_gen_estimators_to_patch\u001b[1;34m()\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_gen_estimators_to_patch\u001b[39m():\n\u001b[0;32m     93\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmlflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     94\u001b[0m         _all_estimators,\n\u001b[0;32m     95\u001b[0m         _get_meta_estimators_for_autologging,\n\u001b[0;32m     96\u001b[0m     )\n\u001b[1;32m---> 98\u001b[0m     _, estimators_to_patch \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m_all_estimators())\n\u001b[0;32m     99\u001b[0m     \u001b[39m# Ensure that relevant meta estimators (e.g. GridSearchCV, Pipeline) are selected\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[39m# for patching if they are not already included in the output of `all_estimators()`\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     estimators_to_patch \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(estimators_to_patch)\u001b[39m.\u001b[39munion(\n\u001b[0;32m    102\u001b[0m         \u001b[39mset\u001b[39m(_get_meta_estimators_for_autologging())\n\u001b[0;32m    103\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\mlflow\\sklearn\\utils.py:858\u001b[0m, in \u001b[0;36m_all_estimators\u001b[1;34m()\u001b[0m\n\u001b[0;32m    855\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    856\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m all_estimators\n\u001b[1;32m--> 858\u001b[0m     \u001b[39mreturn\u001b[39;00m all_estimators()\n\u001b[0;32m    859\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m    860\u001b[0m     \u001b[39mreturn\u001b[39;00m _backported_all_estimators()\n",
      "File \u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\sklearn\\utils\\discovery.py:63\u001b[0m, in \u001b[0;36mall_estimators\u001b[1;34m(type_filter)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39m# Ignore deprecation warnings triggered at import time and from walking\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39m# packages\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[39mwith\u001b[39;00m ignore_warnings(category\u001b[39m=\u001b[39m\u001b[39mFutureWarning\u001b[39;00m):\n\u001b[1;32m---> 63\u001b[0m     \u001b[39mfor\u001b[39;49;00m _, module_name, _ \u001b[39min\u001b[39;49;00m pkgutil\u001b[39m.\u001b[39;49mwalk_packages(path\u001b[39m=\u001b[39;49m[root], prefix\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msklearn.\u001b[39;49m\u001b[39m\"\u001b[39;49m):\n\u001b[0;32m     64\u001b[0m         module_parts \u001b[39m=\u001b[39;49m module_name\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     65\u001b[0m         \u001b[39mif\u001b[39;49;00m (\n\u001b[0;32m     66\u001b[0m             \u001b[39many\u001b[39;49m(part \u001b[39min\u001b[39;49;00m _MODULE_TO_IGNORE \u001b[39mfor\u001b[39;49;00m part \u001b[39min\u001b[39;49;00m module_parts)\n\u001b[0;32m     67\u001b[0m             \u001b[39mor\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39m._\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39min\u001b[39;49;00m module_name\n\u001b[0;32m     68\u001b[0m         ):\n",
      "File \u001b[1;32m~\\ApplicationDev\\miniconda3\\Lib\\pkgutil.py:92\u001b[0m, in \u001b[0;36mwalk_packages\u001b[1;34m(path, prefix, onerror)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[39mif\u001b[39;00m info\u001b[39m.\u001b[39mispkg:\n\u001b[0;32m     91\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 92\u001b[0m         \u001b[39m__import__\u001b[39m(info\u001b[39m.\u001b[39mname)\n\u001b[0;32m     93\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m         \u001b[39mif\u001b[39;00m onerror \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\sklearn\\_build_utils\\__init__.py:15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_min_dependencies\u001b[39;00m \u001b[39mimport\u001b[39;00m CYTHON_MIN_VERSION\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexternals\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_packaging\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m parse\n\u001b[1;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mopenmp_helpers\u001b[39;00m \u001b[39mimport\u001b[39;00m check_openmp_support\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpre_build_helpers\u001b[39;00m \u001b[39mimport\u001b[39;00m basic_check_build\n\u001b[0;32m     18\u001b[0m DEFAULT_ROOT \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msklearn\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\sklearn\\_build_utils\\openmp_helpers.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtextwrap\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpre_build_helpers\u001b[39;00m \u001b[39mimport\u001b[39;00m compile_test_program\n\u001b[0;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_openmp_flag\u001b[39m():\n\u001b[0;32m     16\u001b[0m     \u001b[39mif\u001b[39;00m sys\u001b[39m.\u001b[39mplatform \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwin32\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\sklearn\\_build_utils\\pre_build_helpers.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtempfile\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtextwrap\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msetuptools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommand\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbuild_ext\u001b[39;00m \u001b[39mimport\u001b[39;00m customize_compiler, new_compiler\n\u001b[0;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompile_test_program\u001b[39m(code, extra_preargs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, extra_postargs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     14\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check that some C code can be compiled and run\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\setuptools\\__init__.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39m_distutils_hack\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moverride\u001b[39;00m  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdistutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdistutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39merrors\u001b[39;00m \u001b[39mimport\u001b[39;00m DistutilsOptionError\n",
      "File \u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\override.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39m__import__\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39m_distutils_hack\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mdo_override()\n",
      "File \u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:77\u001b[0m, in \u001b[0;36mdo_override\u001b[1;34m()\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mif\u001b[39;00m enabled():\n\u001b[0;32m     76\u001b[0m     warn_distutils_present()\n\u001b[1;32m---> 77\u001b[0m     ensure_local_distutils()\n",
      "File \u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:64\u001b[0m, in \u001b[0;36mensure_local_distutils\u001b[1;34m()\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39m# check that submodules load as expected\u001b[39;00m\n\u001b[0;32m     63\u001b[0m core \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39m'\u001b[39m\u001b[39mdistutils.core\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 64\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_distutils\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m core\u001b[39m.\u001b[39m\u001b[39m__file__\u001b[39m, core\u001b[39m.\u001b[39m\u001b[39m__file__\u001b[39m\n\u001b[0;32m     65\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39msetuptools._distutils.log\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m sys\u001b[39m.\u001b[39mmodules\n",
      "\u001b[1;31mAssertionError\u001b[0m: C:\\Users\\zacha\\ApplicationDev\\miniconda3\\Lib\\distutils\\core.py"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "mlflow.autolog(log_models=False, exclusive=True)\n",
    "mlflow.sklearn.autolog(log_models=True)\n",
    "\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Changez un ou plusieurs hyperparamètres et relancez un entraînement avec le même nom de modèle. Rendez-vous dans l'onglet model de mlflow que constatez-vous ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "# Create a decision tree classifier with different hyperparameters\n",
    "clf = tree.DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "# Train the classifier\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# Log the model\n",
    "with mlflow.start_run(run_name=\"My Run\"):\n",
    "    mlflow.sklearn.log_model(clf, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez récupérer le dernier run avec la fonction last_active_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Run: data=<RunData: metrics={}, params={}, tags={'mlflow.log-model.history': '[{\"run_id\": \"7340acbfaa82495cbe1affc6d5a20022\", '\n",
      "                             '\"artifact_path\": \"model\", \"utc_time_created\": '\n",
      "                             '\"2023-11-15 09:58:54.453230\", \"flavors\": '\n",
      "                             '{\"python_function\": {\"model_path\": \"model.pkl\", '\n",
      "                             '\"predict_fn\": \"predict\", \"loader_module\": '\n",
      "                             '\"mlflow.sklearn\", \"python_version\": \"3.11.5\", '\n",
      "                             '\"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": '\n",
      "                             '\"python_env.yaml\"}}, \"sklearn\": '\n",
      "                             '{\"pickled_model\": \"model.pkl\", '\n",
      "                             '\"sklearn_version\": \"1.3.2\", '\n",
      "                             '\"serialization_format\": \"cloudpickle\", \"code\": '\n",
      "                             'null}}, \"model_uuid\": '\n",
      "                             '\"62041da82c354743b337efdad4295e45\", '\n",
      "                             '\"mlflow_version\": \"2.8.0\", \"model_size_bytes\": '\n",
      "                             '2090}]',\n",
      " 'mlflow.runName': 'My Run',\n",
      " 'mlflow.source.name': 'c:\\\\Users\\\\zacha\\\\Documents\\\\CourS9\\\\ADDE92Applications '\n",
      "                       'of Big '\n",
      "                       'Data\\\\TP1\\\\envTP1\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py',\n",
      " 'mlflow.source.type': 'LOCAL',\n",
      " 'mlflow.user': 'zacha'}>, info=<RunInfo: artifact_uri='mlflow-artifacts:/636497380083101659/7340acbfaa82495cbe1affc6d5a20022/artifacts', end_time=1700042337126, experiment_id='636497380083101659', lifecycle_stage='active', run_id='7340acbfaa82495cbe1affc6d5a20022', run_name='My Run', run_uuid='7340acbfaa82495cbe1affc6d5a20022', start_time=1700042332398, status='FINISHED', user_id='zacha'>, inputs=<RunInputs: dataset_inputs=[]>>\n"
     ]
    }
   ],
   "source": [
    "run = mlflow.last_active_run() \n",
    "print(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.Entraîner 3,4 modèles avec des hyperparamètres différents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# Define different sets of hyperparameters\n",
    "hyperparameters = [\n",
    "    {\"max_depth\": 3, \"min_samples_split\": 2},\n",
    "    {\"max_depth\": 5, \"min_samples_split\": 4},\n",
    "    {\"max_depth\": None, \"min_samples_split\": 2},\n",
    "    {\"max_depth\": 7, \"min_samples_split\": 10}\n",
    "]\n",
    "\n",
    "# Train a model for each set of hyperparameters\n",
    "for i, params in enumerate(hyperparameters):\n",
    "    # Create a decision tree classifier with the current hyperparameters\n",
    "    clf = tree.DecisionTreeClassifier(max_depth=params[\"max_depth\"], min_samples_split=params[\"min_samples_split\"])\n",
    "\n",
    "    # Train the classifier\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "\n",
    "    # Log the model\n",
    "    with mlflow.start_run(run_name=f\"My Run {i+1}\"):\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.sklearn.log_model(clf, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Versionning des données avec DVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque run le dataset utilisé est précisé dans mlflow. Mais les informations fournies sont assez pauvres. Pour améliorer ça nous allons utiliser l'outil dvc en combinaison avec mlflow.\n",
    "dvc fonctionne de pair avec git."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisez un repo git localement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized empty Git repository in C:/Users/zacha/Documents/CourS9/ADDE92Applications of Big Data/TP1/.git/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hint: Using 'master' as the name for the initial branch. This default branch name\n",
      "hint: is subject to change. To configure the initial branch name to use in all\n",
      "hint: of your new repositories, which will suppress this warning, call:\n",
      "hint: \n",
      "hint: \tgit config --global init.defaultBranch <name>\n",
      "hint: \n",
      "hint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\n",
      "hint: 'development'. The just-created branch can be renamed via this command:\n",
      "hint: \n",
      "hint: \tgit branch -m <name>\n"
     ]
    }
   ],
   "source": [
    "!git init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installez dvc avec pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dvc\n",
      "  Obtaining dependency information for dvc from https://files.pythonhosted.org/packages/e4/2b/2e04e73dbd9f87f5f4b3427e5053fe6c91f87e04c9028d704d91160f018a/dvc-3.29.0-py3-none-any.whl.metadata\n",
      "  Downloading dvc-3.29.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: colorama>=0.3.9 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from dvc) (0.4.6)\n",
      "Collecting configobj>=5.0.6 (from dvc)\n",
      "  Downloading configobj-5.0.8-py2.py3-none-any.whl (36 kB)\n",
      "Collecting distro>=1.3 (from dvc)\n",
      "  Using cached distro-1.8.0-py3-none-any.whl (20 kB)\n",
      "Collecting dpath<3,>=2.1.0 (from dvc)\n",
      "  Obtaining dependency information for dpath<3,>=2.1.0 from https://files.pythonhosted.org/packages/84/c8/10c2d41a0958b76e777c07a521d64c871ab9022520babb3e08fa7eeb0810/dpath-2.1.6-py3-none-any.whl.metadata\n",
      "  Downloading dpath-2.1.6-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting dvc-data<2.21.0,>=2.20.0 (from dvc)\n",
      "  Obtaining dependency information for dvc-data<2.21.0,>=2.20.0 from https://files.pythonhosted.org/packages/d1/f3/0b62e3c2711ed3647032176f44482e8c19ba7925e66fb53fdfd7f6ca4b1f/dvc_data-2.20.0-py3-none-any.whl.metadata\n",
      "  Downloading dvc_data-2.20.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting dvc-http>=2.29.0 (from dvc)\n",
      "  Downloading dvc_http-2.30.2-py3-none-any.whl (12 kB)\n",
      "Collecting dvc-render<1,>=0.3.1 (from dvc)\n",
      "  Obtaining dependency information for dvc-render<1,>=0.3.1 from https://files.pythonhosted.org/packages/e4/52/675239b9451c327a462fc66a09f4c1c96a5b5d90faf566864e35e15a7791/dvc_render-0.6.0-py3-none-any.whl.metadata\n",
      "  Downloading dvc_render-0.6.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting dvc-studio-client<1,>=0.13.0 (from dvc)\n",
      "  Obtaining dependency information for dvc-studio-client<1,>=0.13.0 from https://files.pythonhosted.org/packages/54/39/93df103f61f26fabcc4322226bb8aa187b945a309a14ca32e8fc4c3f41d1/dvc_studio_client-0.15.0-py3-none-any.whl.metadata\n",
      "  Downloading dvc_studio_client-0.15.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting dvc-task<1,>=0.3.0 (from dvc)\n",
      "  Obtaining dependency information for dvc-task<1,>=0.3.0 from https://files.pythonhosted.org/packages/93/8c/3aebe5c887c1277b671aa9d3931a836375ed2443ad37302c50d0ae84c50d/dvc_task-0.3.0-py3-none-any.whl.metadata\n",
      "  Downloading dvc_task-0.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting flatten-dict<1,>=0.4.1 (from dvc)\n",
      "  Downloading flatten_dict-0.4.2-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting flufl.lock<8,>=5 (from dvc)\n",
      "  Downloading flufl.lock-7.1.1-py3-none-any.whl (11 kB)\n",
      "Collecting funcy>=1.14 (from dvc)\n",
      "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
      "Collecting grandalf<1,>=0.7 (from dvc)\n",
      "  Downloading grandalf-0.8-py3-none-any.whl (41 kB)\n",
      "     ---------------------------------------- 0.0/41.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.8/41.8 kB ? eta 0:00:00\n",
      "Collecting gto<2,>=1.4.0 (from dvc)\n",
      "  Obtaining dependency information for gto<2,>=1.4.0 from https://files.pythonhosted.org/packages/8d/e4/0fd9dc427bcf2e009e3d2272d6704a0551597a30e4a5b984b2c2f9784cc0/gto-1.5.0-py3-none-any.whl.metadata\n",
      "  Downloading gto-1.5.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting hydra-core>=1.1 (from dvc)\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "     ---------------------------------------- 0.0/154.5 kB ? eta -:--:--\n",
      "     ----------------------------------- -- 143.4/154.5 kB 8.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 154.5/154.5 kB 4.5 MB/s eta 0:00:00\n",
      "Collecting iterative-telemetry>=0.0.7 (from dvc)\n",
      "  Downloading iterative_telemetry-0.0.8-py3-none-any.whl (10 kB)\n",
      "Collecting networkx>=2.5 (from dvc)\n",
      "  Obtaining dependency information for networkx>=2.5 from https://files.pythonhosted.org/packages/d5/f0/8fbc882ca80cf077f1b246c0e3c3465f7f415439bdea6b899f6b19f61f70/networkx-3.2.1-py3-none-any.whl.metadata\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: packaging>=19 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from dvc) (23.2)\n",
      "Collecting pathspec>=0.10.3 (from dvc)\n",
      "  Obtaining dependency information for pathspec>=0.10.3 from https://files.pythonhosted.org/packages/b4/2a/9b1be29146139ef459188f5e420a66e835dda921208db600b7037093891f/pathspec-0.11.2-py3-none-any.whl.metadata\n",
      "  Downloading pathspec-0.11.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting platformdirs<4,>=3.1.1 (from dvc)\n",
      "  Obtaining dependency information for platformdirs<4,>=3.1.1 from https://files.pythonhosted.org/packages/56/29/3ec311dc18804409ecf0d2b09caa976f3ae6215559306b5b530004e11156/platformdirs-3.11.0-py3-none-any.whl.metadata\n",
      "  Using cached platformdirs-3.11.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: psutil>=5.8 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from dvc) (5.9.6)\n",
      "Collecting pydot>=1.2.4 (from dvc)\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting pygtrie>=2.3.2 (from dvc)\n",
      "  Downloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: pyparsing>=2.4.7 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from dvc) (3.1.1)\n",
      "Requirement already satisfied: requests>=2.22 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from dvc) (2.31.0)\n",
      "Collecting rich>=12 (from dvc)\n",
      "  Obtaining dependency information for rich>=12 from https://files.pythonhosted.org/packages/be/2a/4e62ff633612f746f88618852a626bbe24226eba5e7ac90e91dcfd6a414e/rich-13.6.0-py3-none-any.whl.metadata\n",
      "  Downloading rich-13.6.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting ruamel.yaml>=0.17.11 (from dvc)\n",
      "  Obtaining dependency information for ruamel.yaml>=0.17.11 from https://files.pythonhosted.org/packages/57/db/4b74a29f5fd175aea3ff0d95f8230d9bb8a54e38d963c6f96065b9e2eef3/ruamel.yaml-0.18.5-py3-none-any.whl.metadata\n",
      "  Downloading ruamel.yaml-0.18.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting scmrepo<2,>=1.4.1 (from dvc)\n",
      "  Obtaining dependency information for scmrepo<2,>=1.4.1 from https://files.pythonhosted.org/packages/65/97/d8bb26cf1aaa09fcaae4d98d5e46ae2868114c8264e05070a61fefbe6645/scmrepo-1.4.1-py3-none-any.whl.metadata\n",
      "  Downloading scmrepo-1.4.1-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting shortuuid>=0.5 (from dvc)\n",
      "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
      "Collecting shtab<2,>=1.3.4 (from dvc)\n",
      "  Obtaining dependency information for shtab<2,>=1.3.4 from https://files.pythonhosted.org/packages/86/69/3a4873b36d65a1b8f4ee606f5a785b5babb9960385802de60d8455e2f8b6/shtab-1.6.4-py3-none-any.whl.metadata\n",
      "  Downloading shtab-1.6.4-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: tabulate>=0.8.7 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from dvc) (0.9.0)\n",
      "Collecting tomlkit>=0.11.1 (from dvc)\n",
      "  Obtaining dependency information for tomlkit>=0.11.1 from https://files.pythonhosted.org/packages/6e/43/159750d32481f16e34cc60090b53bc0a14314ad0c1f67a9bb64f3f3a0551/tomlkit-0.12.3-py3-none-any.whl.metadata\n",
      "  Downloading tomlkit-0.12.3-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting tqdm<5,>=4.63.1 (from dvc)\n",
      "  Obtaining dependency information for tqdm<5,>=4.63.1 from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting voluptuous>=0.11.7 (from dvc)\n",
      "  Obtaining dependency information for voluptuous>=0.11.7 from https://files.pythonhosted.org/packages/ce/03/7b91e5fa5b36947cb858c45bc50270fee86b3db6c2cd002eac2ee68c09a1/voluptuous-0.14.0-py3-none-any.whl.metadata\n",
      "  Downloading voluptuous-0.14.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting zc.lockfile>=1.2.1 (from dvc)\n",
      "  Downloading zc.lockfile-3.0.post1-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: six in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from configobj>=5.0.6->dvc) (1.16.0)\n",
      "Collecting dictdiffer>=0.8.1 (from dvc-data<2.21.0,>=2.20.0->dvc)\n",
      "  Downloading dictdiffer-0.9.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting dvc-objects<2,>=1.1.0 (from dvc-data<2.21.0,>=2.20.0->dvc)\n",
      "  Obtaining dependency information for dvc-objects<2,>=1.1.0 from https://files.pythonhosted.org/packages/cb/da/f849b088d313772e60be2db3268100d9e06b5c3530c10077529894cd5d5e/dvc_objects-1.2.0-py3-none-any.whl.metadata\n",
      "  Downloading dvc_objects-1.2.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting diskcache>=5.2.1 (from dvc-data<2.21.0,>=2.20.0->dvc)\n",
      "  Obtaining dependency information for diskcache>=5.2.1 from https://files.pythonhosted.org/packages/3f/27/4570e78fc0bf5ea0ca45eb1de3818a23787af9b390c0b0a0033a1b8236f9/diskcache-5.6.3-py3-none-any.whl.metadata\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting attrs>=21.3.0 (from dvc-data<2.21.0,>=2.20.0->dvc)\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "     ---------------------------------------- 0.0/61.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 61.2/61.2 kB ? eta 0:00:00\n",
      "Collecting sqltrie<1,>=0.8.0 (from dvc-data<2.21.0,>=2.20.0->dvc)\n",
      "  Obtaining dependency information for sqltrie<1,>=0.8.0 from https://files.pythonhosted.org/packages/ae/c8/2a590d30003d2c0a66571dc0c3fb2b143bffaf93fbc14f80c0489afb889e/sqltrie-0.8.0-py3-none-any.whl.metadata\n",
      "  Downloading sqltrie-0.8.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting fsspec[http] (from dvc-http>=2.29.0->dvc)\n",
      "  Obtaining dependency information for fsspec[http] from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n",
      "  Using cached fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp-retry>=2.5.0 (from dvc-http>=2.29.0->dvc)\n",
      "  Downloading aiohttp_retry-2.8.3-py3-none-any.whl (9.8 kB)\n",
      "Collecting dulwich (from dvc-studio-client<1,>=0.13.0->dvc)\n",
      "  Obtaining dependency information for dulwich from https://files.pythonhosted.org/packages/59/f6/1c2491bc270d7b66157c1f13203d2325ba0a210cfd62f600fab2ed16f6f3/dulwich-0.21.6-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading dulwich-0.21.6-cp311-cp311-win_amd64.whl.metadata (4.4 kB)\n",
      "Collecting celery<6,>=5.3.0 (from dvc-task<1,>=0.3.0->dvc)\n",
      "  Obtaining dependency information for celery<6,>=5.3.0 from https://files.pythonhosted.org/packages/a3/fb/0bcea0312649a374601d5a15092b2a3659801d578e70cec03aa72053ccaa/celery-5.3.5-py3-none-any.whl.metadata\n",
      "  Downloading celery-5.3.5-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting kombu<6,>=5.3.0 (from dvc-task<1,>=0.3.0->dvc)\n",
      "  Obtaining dependency information for kombu<6,>=5.3.0 from https://files.pythonhosted.org/packages/c4/05/0504dce43327e610cbe05c223691992bdb0212202bb830f105d57641665f/kombu-5.3.3-py3-none-any.whl.metadata\n",
      "  Downloading kombu-5.3.3-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: pywin32>=225 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from dvc-task<1,>=0.3.0->dvc) (306)\n",
      "Collecting atpublic>=2.3 (from flufl.lock<8,>=5->dvc)\n",
      "  Obtaining dependency information for atpublic>=2.3 from https://files.pythonhosted.org/packages/42/d5/f3c7110d3763af646150203b8bfe6932ab05a9b3e228c27d138babeb92ae/atpublic-4.0-py3-none-any.whl.metadata\n",
      "  Downloading atpublic-4.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting typer>=0.4.1 (from gto<2,>=1.4.0->dvc)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 0.0/45.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 45.9/45.9 kB ? eta 0:00:00\n",
      "Collecting pydantic!=2.0.0,<3,>=1.9.0 (from gto<2,>=1.4.0->dvc)\n",
      "  Obtaining dependency information for pydantic!=2.0.0,<3,>=1.9.0 from https://files.pythonhosted.org/packages/d7/10/ddfb9539a6e55f7dfd6c2b9b81d86fcba2761ba87eeb81f8b1012957dcdc/pydantic-2.5.0-py3-none-any.whl.metadata\n",
      "  Downloading pydantic-2.5.0-py3-none-any.whl.metadata (174 kB)\n",
      "     ---------------------------------------- 0.0/174.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 174.6/174.6 kB 5.3 MB/s eta 0:00:00\n",
      "Collecting semver>=3.0.0 (from gto<2,>=1.4.0->dvc)\n",
      "  Obtaining dependency information for semver>=3.0.0 from https://files.pythonhosted.org/packages/9a/77/0cc7a8a3bc7e53d07e8f47f147b92b0960e902b8254859f4aee5c4d7866b/semver-3.0.2-py3-none-any.whl.metadata\n",
      "  Downloading semver-3.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from gto<2,>=1.4.0->dvc) (0.4)\n",
      "Collecting omegaconf<2.4,>=2.2 (from hydra-core>=1.1->dvc)\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "     ---------------------------------------- 0.0/79.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 79.5/79.5 kB 4.6 MB/s eta 0:00:00\n",
      "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1->dvc)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "     ---------------------------------------- 0.0/117.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 117.0/117.0 kB 7.1 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting appdirs (from iterative-telemetry>=0.0.7->dvc)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting filelock (from iterative-telemetry>=0.0.7->dvc)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/81/54/84d42a0bee35edba99dee7b59a8d4970eccdd44b99fe728ed912106fc781/filelock-3.13.1-py3-none-any.whl.metadata\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from requests>=2.22->dvc) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from requests>=2.22->dvc) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from requests>=2.22->dvc) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from requests>=2.22->dvc) (2023.7.22)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=12->dvc)\n",
      "  Obtaining dependency information for markdown-it-py>=2.2.0 from https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl.metadata\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from rich>=12->dvc) (2.16.1)\n",
      "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.11->dvc)\n",
      "  Obtaining dependency information for ruamel.yaml.clib>=0.2.7 from https://files.pythonhosted.org/packages/ec/54/d8a795997921d87224c65d44499ca595a833093fb215b133f920c1062956/ruamel.yaml.clib-0.2.8-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading ruamel.yaml.clib-0.2.8-cp311-cp311-win_amd64.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: gitpython>3 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from scmrepo<2,>=1.4.1->dvc) (3.1.40)\n",
      "Collecting pygit2>=1.13.0 (from scmrepo<2,>=1.4.1->dvc)\n",
      "  Obtaining dependency information for pygit2>=1.13.0 from https://files.pythonhosted.org/packages/95/dc/86f76afa17ed49cef40373f9f2b7c3bb90615fde8983c0de1ed1a4acdc50/pygit2-1.13.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading pygit2-1.13.2-cp311-cp311-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting asyncssh<3,>=2.13.1 (from scmrepo<2,>=1.4.1->dvc)\n",
      "  Obtaining dependency information for asyncssh<3,>=2.13.1 from https://files.pythonhosted.org/packages/9f/42/72736d020c5bf2e2271d119e5907a41d871d2d0dacc118b2cb3d82b4fcf4/asyncssh-2.14.1-py3-none-any.whl.metadata\n",
      "  Downloading asyncssh-2.14.1-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from zc.lockfile>=1.2.1->dvc) (68.2.2)\n",
      "Collecting aiohttp (from aiohttp-retry>=2.5.0->dvc-http>=2.29.0->dvc)\n",
      "  Obtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/2e/9f/9c37b01fc6a37c92f139a4cd937a92f03ebbd75379cfd55e85ca1e571643/aiohttp-3.8.6-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading aiohttp-3.8.6-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting cryptography>=39.0 (from asyncssh<3,>=2.13.1->scmrepo<2,>=1.4.1->dvc)\n",
      "  Obtaining dependency information for cryptography>=39.0 from https://files.pythonhosted.org/packages/86/35/f03a42444866ef7f23134812a05012dcb509418214fb78ec848f28cd14b8/cryptography-41.0.5-cp37-abi3-win_amd64.whl.metadata\n",
      "  Using cached cryptography-41.0.5-cp37-abi3-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from asyncssh<3,>=2.13.1->scmrepo<2,>=1.4.1->dvc) (4.8.0)\n",
      "Collecting billiard<5.0,>=4.2.0 (from celery<6,>=5.3.0->dvc-task<1,>=0.3.0->dvc)\n",
      "  Obtaining dependency information for billiard<5.0,>=4.2.0 from https://files.pythonhosted.org/packages/50/8d/6e9fdeeab04d803abc5a715175f87e88893934d5590595eacff23ca12b07/billiard-4.2.0-py3-none-any.whl.metadata\n",
      "  Downloading billiard-4.2.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting click-didyoumean>=0.3.0 (from celery<6,>=5.3.0->dvc-task<1,>=0.3.0->dvc)\n",
      "  Downloading click_didyoumean-0.3.0-py3-none-any.whl (2.7 kB)\n",
      "Collecting click-plugins>=1.1.1 (from celery<6,>=5.3.0->dvc-task<1,>=0.3.0->dvc)\n",
      "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting click-repl>=0.2.0 (from celery<6,>=5.3.0->dvc-task<1,>=0.3.0->dvc)\n",
      "  Obtaining dependency information for click-repl>=0.2.0 from https://files.pythonhosted.org/packages/52/40/9d857001228658f0d59e97ebd4c346fe73e138c6de1bce61dc568a57c7f8/click_repl-0.3.0-py3-none-any.whl.metadata\n",
      "  Downloading click_repl-0.3.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: click<9.0,>=8.1.2 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from celery<6,>=5.3.0->dvc-task<1,>=0.3.0->dvc) (8.1.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from celery<6,>=5.3.0->dvc-task<1,>=0.3.0->dvc) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from celery<6,>=5.3.0->dvc-task<1,>=0.3.0->dvc) (2023.3)\n",
      "Collecting vine<6.0,>=5.1.0 (from celery<6,>=5.3.0->dvc-task<1,>=0.3.0->dvc)\n",
      "  Obtaining dependency information for vine<6.0,>=5.1.0 from https://files.pythonhosted.org/packages/03/ff/7c0c86c43b3cbb927e0ccc0255cb4057ceba4799cd44ae95174ce8e8b5b2/vine-5.1.0-py3-none-any.whl.metadata\n",
      "  Downloading vine-5.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from gitpython>3->scmrepo<2,>=1.4.1->dvc) (4.0.11)\n",
      "Collecting amqp<6.0.0,>=5.1.1 (from kombu<6,>=5.3.0->dvc-task<1,>=0.3.0->dvc)\n",
      "  Obtaining dependency information for amqp<6.0.0,>=5.1.1 from https://files.pythonhosted.org/packages/b3/f0/8e5be5d5e0653d9e1d02b1144efa33ff7d2963dfad07049e02c0fa9b2e8d/amqp-5.2.0-py3-none-any.whl.metadata\n",
      "  Downloading amqp-5.2.0-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=12->dvc)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from omegaconf<2.4,>=2.2->hydra-core>=1.1->dvc) (6.0.1)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=2.0.0,<3,>=1.9.0->gto<2,>=1.4.0->dvc)\n",
      "  Obtaining dependency information for annotated-types>=0.4.0 from https://files.pythonhosted.org/packages/28/78/d31230046e58c207284c6b2c4e8d96e6d3cb4e52354721b944d3e1ee4aa5/annotated_types-0.6.0-py3-none-any.whl.metadata\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.14.1 (from pydantic!=2.0.0,<3,>=1.9.0->gto<2,>=1.4.0->dvc)\n",
      "  Obtaining dependency information for pydantic-core==2.14.1 from https://files.pythonhosted.org/packages/15/8e/68d3a522ac31e782e3ae95ddc7b4ec4bea0f5c552a98f85abdee6696a607/pydantic_core-2.14.1-cp311-none-win_amd64.whl.metadata\n",
      "  Downloading pydantic_core-2.14.1-cp311-none-win_amd64.whl.metadata (6.6 kB)\n",
      "Collecting cffi>=1.16.0 (from pygit2>=1.13.0->scmrepo<2,>=1.4.1->dvc)\n",
      "  Obtaining dependency information for cffi>=1.16.0 from https://files.pythonhosted.org/packages/5a/c7/694814b3757878b29da39bc2f0cf9d20295f4c1e0a0bde7971708d5f23f8/cffi-1.16.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached cffi-1.16.0-cp311-cp311-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting orjson (from sqltrie<1,>=0.8.0->dvc-data<2.21.0,>=2.20.0->dvc)\n",
      "  Obtaining dependency information for orjson from https://files.pythonhosted.org/packages/5d/67/d7837cf0ac956e3c81c67dda3e8f2ffc60dd50ffc480ec7c17f2e22a36ae/orjson-3.9.10-cp311-none-win_amd64.whl.metadata\n",
      "  Downloading orjson-3.9.10-cp311-none-win_amd64.whl.metadata (50 kB)\n",
      "     ---------------------------------------- 0.0/50.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 50.5/50.5 kB 2.5 MB/s eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->aiohttp-retry>=2.5.0->dvc-http>=2.29.0->dvc)\n",
      "  Downloading multidict-6.0.4-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->aiohttp-retry>=2.5.0->dvc-http>=2.29.0->dvc)\n",
      "  Obtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->aiohttp-retry>=2.5.0->dvc-http>=2.29.0->dvc)\n",
      "  Downloading yarl-1.9.2-cp311-cp311-win_amd64.whl (60 kB)\n",
      "     ---------------------------------------- 0.0/60.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.2/60.2 kB ? eta 0:00:00\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->aiohttp-retry>=2.5.0->dvc-http>=2.29.0->dvc)\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/39/16/72d9ccd30815d0b37218348f053be37bc3d14288ac192a794a39990ac28e/frozenlist-1.4.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading frozenlist-1.4.0-cp311-cp311-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->aiohttp-retry>=2.5.0->dvc-http>=2.29.0->dvc)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting pycparser (from cffi>=1.16.0->pygit2>=1.13.0->scmrepo<2,>=1.4.1->dvc)\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.36 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from click-repl>=0.2.0->celery<6,>=5.3.0->dvc-task<1,>=0.3.0->dvc) (3.0.41)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython>3->scmrepo<2,>=1.4.1->dvc) (5.0.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from prompt-toolkit>=3.0.36->click-repl>=0.2.0->celery<6,>=5.3.0->dvc-task<1,>=0.3.0->dvc) (0.2.10)\n",
      "Downloading dvc-3.29.0-py3-none-any.whl (429 kB)\n",
      "   ---------------------------------------- 0.0/429.9 kB ? eta -:--:--\n",
      "   -------------------------------- ------ 358.4/429.9 kB 11.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 429.9/429.9 kB 13.5 MB/s eta 0:00:00\n",
      "Downloading dpath-2.1.6-py3-none-any.whl (17 kB)\n",
      "Downloading dvc_data-2.20.0-py3-none-any.whl (68 kB)\n",
      "   ---------------------------------------- 0.0/68.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 68.0/68.0 kB ? eta 0:00:00\n",
      "Downloading dvc_render-0.6.0-py3-none-any.whl (19 kB)\n",
      "Downloading dvc_studio_client-0.15.0-py3-none-any.whl (13 kB)\n",
      "Downloading dvc_task-0.3.0-py3-none-any.whl (21 kB)\n",
      "Downloading gto-1.5.0-py3-none-any.whl (46 kB)\n",
      "   ---------------------------------------- 0.0/46.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 46.6/46.6 kB ? eta 0:00:00\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Downloading pathspec-0.11.2-py3-none-any.whl (29 kB)\n",
      "Using cached platformdirs-3.11.0-py3-none-any.whl (17 kB)\n",
      "Downloading rich-13.6.0-py3-none-any.whl (239 kB)\n",
      "   ---------------------------------------- 0.0/239.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 239.8/239.8 kB 15.3 MB/s eta 0:00:00\n",
      "Downloading ruamel.yaml-0.18.5-py3-none-any.whl (116 kB)\n",
      "   ---------------------------------------- 0.0/116.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 116.4/116.4 kB 6.6 MB/s eta 0:00:00\n",
      "Downloading scmrepo-1.4.1-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.0/58.0 kB ? eta 0:00:00\n",
      "Downloading shtab-1.6.4-py3-none-any.whl (13 kB)\n",
      "Downloading tomlkit-0.12.3-py3-none-any.whl (37 kB)\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Downloading voluptuous-0.14.0-py3-none-any.whl (30 kB)\n",
      "Downloading asyncssh-2.14.1-py3-none-any.whl (352 kB)\n",
      "   ---------------------------------------- 0.0/352.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 352.2/352.2 kB 11.0 MB/s eta 0:00:00\n",
      "Downloading atpublic-4.0-py3-none-any.whl (4.9 kB)\n",
      "Downloading celery-5.3.5-py3-none-any.whl (421 kB)\n",
      "   ---------------------------------------- 0.0/421.9 kB ? eta -:--:--\n",
      "   -------------------------- ------------ 286.7/421.9 kB 17.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 421.9/421.9 kB 8.8 MB/s eta 0:00:00\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.5/45.5 kB ? eta 0:00:00\n",
      "Downloading dulwich-0.21.6-cp311-cp311-win_amd64.whl (484 kB)\n",
      "   ---------------------------------------- 0.0/484.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 484.9/484.9 kB 31.6 MB/s eta 0:00:00\n",
      "Downloading dvc_objects-1.2.0-py3-none-any.whl (38 kB)\n",
      "Using cached fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "Downloading kombu-5.3.3-py3-none-any.whl (199 kB)\n",
      "   ---------------------------------------- 0.0/199.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 199.1/199.1 kB 12.6 MB/s eta 0:00:00\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "   ---------------------------------------- 0.0/87.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 87.5/87.5 kB ? eta 0:00:00\n",
      "Downloading pydantic-2.5.0-py3-none-any.whl (407 kB)\n",
      "   ---------------------------------------- 0.0/407.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 407.5/407.5 kB 24.8 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.14.1-cp311-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 0.9/1.9 MB 27.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 19.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 20.0 MB/s eta 0:00:00\n",
      "Downloading pygit2-1.13.2-cp311-cp311-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 1.1/1.2 MB 33.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 19.2 MB/s eta 0:00:00\n",
      "Downloading ruamel.yaml.clib-0.2.8-cp311-cp311-win_amd64.whl (118 kB)\n",
      "   ---------------------------------------- 0.0/118.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 118.0/118.0 kB ? eta 0:00:00\n",
      "Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
      "Downloading sqltrie-0.8.0-py3-none-any.whl (17 kB)\n",
      "Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading aiohttp-3.8.6-cp311-cp311-win_amd64.whl (322 kB)\n",
      "   ---------------------------------------- 0.0/322.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 322.6/322.6 kB ? eta 0:00:00\n",
      "Downloading amqp-5.2.0-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.9/50.9 kB ? eta 0:00:00\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading billiard-4.2.0-py3-none-any.whl (86 kB)\n",
      "   ---------------------------------------- 0.0/86.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 86.7/86.7 kB ? eta 0:00:00\n",
      "Using cached cffi-1.16.0-cp311-cp311-win_amd64.whl (181 kB)\n",
      "Downloading click_repl-0.3.0-py3-none-any.whl (10 kB)\n",
      "Using cached cryptography-41.0.5-cp37-abi3-win_amd64.whl (2.7 MB)\n",
      "Downloading vine-5.1.0-py3-none-any.whl (9.6 kB)\n",
      "Downloading orjson-3.9.10-cp311-none-win_amd64.whl (135 kB)\n",
      "   ---------------------------------------- 0.0/135.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 135.0/135.0 kB 7.8 MB/s eta 0:00:00\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 44.9/44.9 kB ? eta 0:00:00\n",
      "Building wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (pyproject.toml): started\n",
      "  Building wheel for antlr4-python3-runtime (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144579 sha256=e4cbdd1cd74b0e2b742fe27f77532a72ef62646541692faa43fd557c280d9393\n",
      "  Stored in directory: c:\\users\\zacha\\appdata\\local\\pip\\cache\\wheels\\1a\\97\\32\\461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: voluptuous, pygtrie, funcy, dictdiffer, appdirs, antlr4-python3-runtime, zc.lockfile, vine, tqdm, tomlkit, shtab, shortuuid, semver, ruamel.yaml.clib, pydot, pydantic-core, pycparser, platformdirs, pathspec, orjson, omegaconf, networkx, multidict, mdurl, grandalf, fsspec, frozenlist, flatten-dict, filelock, dvc-render, dulwich, dpath, distro, diskcache, configobj, billiard, attrs, atpublic, async-timeout, annotated-types, yarl, typer, sqltrie, ruamel.yaml, pydantic, markdown-it-py, iterative-telemetry, hydra-core, flufl.lock, dvc-studio-client, dvc-objects, click-repl, click-plugins, click-didyoumean, cffi, amqp, aiosignal, rich, pygit2, kombu, dvc-data, cryptography, aiohttp, celery, asyncssh, aiohttp-retry, scmrepo, dvc-task, dvc-http, gto, dvc\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 4.0.0\n",
      "    Uninstalling platformdirs-4.0.0:\n",
      "      Successfully uninstalled platformdirs-4.0.0\n",
      "Successfully installed aiohttp-3.8.6 aiohttp-retry-2.8.3 aiosignal-1.3.1 amqp-5.2.0 annotated-types-0.6.0 antlr4-python3-runtime-4.9.3 appdirs-1.4.4 async-timeout-4.0.3 asyncssh-2.14.1 atpublic-4.0 attrs-23.1.0 billiard-4.2.0 celery-5.3.5 cffi-1.16.0 click-didyoumean-0.3.0 click-plugins-1.1.1 click-repl-0.3.0 configobj-5.0.8 cryptography-41.0.5 dictdiffer-0.9.0 diskcache-5.6.3 distro-1.8.0 dpath-2.1.6 dulwich-0.21.6 dvc-3.29.0 dvc-data-2.20.0 dvc-http-2.30.2 dvc-objects-1.2.0 dvc-render-0.6.0 dvc-studio-client-0.15.0 dvc-task-0.3.0 filelock-3.13.1 flatten-dict-0.4.2 flufl.lock-7.1.1 frozenlist-1.4.0 fsspec-2023.10.0 funcy-2.0 grandalf-0.8 gto-1.5.0 hydra-core-1.3.2 iterative-telemetry-0.0.8 kombu-5.3.3 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.0.4 networkx-3.2.1 omegaconf-2.3.0 orjson-3.9.10 pathspec-0.11.2 platformdirs-3.11.0 pycparser-2.21 pydantic-2.5.0 pydantic-core-2.14.1 pydot-1.4.2 pygit2-1.13.2 pygtrie-2.5.0 rich-13.6.0 ruamel.yaml-0.18.5 ruamel.yaml.clib-0.2.8 scmrepo-1.4.1 semver-3.0.2 shortuuid-1.0.11 shtab-1.6.4 sqltrie-0.8.0 tomlkit-0.12.3 tqdm-4.66.1 typer-0.9.0 vine-5.1.0 voluptuous-0.14.0 yarl-1.9.2 zc.lockfile-3.0.post1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install dvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisez un projet dvc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized DVC repository.\n",
      "\n",
      "You can now commit the changes to git.\n",
      "\n",
      "+---------------------------------------------------------------------+\n",
      "|                                                                     |\n",
      "|        DVC has enabled anonymous aggregate usage analytics.         |\n",
      "|     Read the analytics documentation (and how to opt-out) here:     |\n",
      "|             <https://dvc.org/doc/user-guide/analytics>              |\n",
      "|                                                                     |\n",
      "+---------------------------------------------------------------------+\n",
      "\n",
      "What's next?\n",
      "------------\n",
      "- Check out the documentation: <https://dvc.org/doc>\n",
      "- Get help and share ideas: <https://dvc.org/chat>\n",
      "- Star us on GitHub: <https://github.com/iterative/dvc>\n"
     ]
    }
   ],
   "source": [
    "!dvc init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardez ce qui a été créé avec un git status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\n",
      "Your branch is up to date with 'origin/master'.\n",
      "\n",
      "Changes to be committed:\n",
      "  (use \"git restore --staged <file>...\" to unstage)\n",
      "\tnew file:   .dvc/.gitignore\n",
      "\tnew file:   .dvc/config\n",
      "\tnew file:   .dvcignore\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajoutez les nouveaux fichiers dans le fichier .dvc : le .gitignore, le fichier config, et le .dvcignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 1d98e7b] initialize repo\n",
      " 3 files changed, 6 insertions(+)\n",
      " create mode 100644 .dvc/.gitignore\n",
      " create mode 100644 .dvc/config\n",
      " create mode 100644 .dvcignore\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"initialize repo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons ajouter un stockage distant dans le cloud. Ici nous allons utiliser un stockage dans un dossier local pour les besoins du tp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting 'dvc-remote' as a default remote.\n"
     ]
    }
   ],
   "source": [
    "!dvc remote add -d dvc-remote /tmp/dvc-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut regarder le contenu du fichier dvc config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[core]\n",
      "    remote = dvc-remote\n",
      "['remote \"dvc-remote\"']\n",
      "    url = /tmp/dvc-storage\n"
     ]
    }
   ],
   "source": [
    "!type .dvc\\config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous constatons que l'url locale a bien été ajouté. Nous pouvons commiter ces changements à git."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 819acdb] add remote storage\n",
      " 1 file changed, 4 insertions(+)\n"
     ]
    }
   ],
   "source": [
    "!git commit .dvc/config -m \"add remote storage\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons créer un dossier data pour stocker notre dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Télécharger notre dataset et le placer dans le dossier data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataset_source_url = \"https://archive.ics.uci.edu/static/public/275/bike+sharing+dataset.zip\"\n",
    "\n",
    "content = requests.get(dataset_source_url).content\n",
    "with zipfile.ZipFile(io.BytesIO(content)) as arc:\n",
    "    raw_data = pd.read_csv(arc.open(\"hour.csv\"), header=0, sep=',', parse_dates=['dteday'], index_col='dteday')\n",
    "\n",
    "raw_data.to_csv(path_or_buf=\"data/hour.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichons le contenu de data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C s'appelle OS\n",
      " Le num�ro de s�rie du volume est 506E-5B85\n",
      "\n",
      " R�pertoire de c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\data\n",
      "\n",
      "15/11/2023  11:06    <DIR>          .\n",
      "15/11/2023  11:05    <DIR>          ..\n",
      "15/11/2023  11:06         1�161�688 hour.csv\n",
      "               1 fichier(s)        1�161�688 octets\n",
      "               2 R�p(s)  340�687�327�232 octets libres\n"
     ]
    }
   ],
   "source": [
    "!dir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nous voulons commencer à suivre les changements d'un fichier il nous suffit de l'ajouter via dvc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add 'data\\.gitignore' 'data\\hour.csv.dvc'\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⠋ Checking graph\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!dvc add data/hour.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons regarder de nouveau ce qu'il y a dans notre dossier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C s'appelle OS\n",
      " Le num�ro de s�rie du volume est 506E-5B85\n",
      "\n",
      " R�pertoire de c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\data\n",
      "\n",
      "15/11/2023  11:06    <DIR>          .\n",
      "15/11/2023  11:05    <DIR>          ..\n",
      "15/11/2023  11:06                11 .gitignore\n",
      "15/11/2023  11:06         1�161�688 hour.csv\n",
      "15/11/2023  11:06                96 hour.csv.dvc\n",
      "               3 fichier(s)        1�161�795 octets\n",
      "               2 R�p(s)  340�685�553�664 octets libres\n"
     ]
    }
   ],
   "source": [
    "!dir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voyons un nouveau fichier .dvc.   \n",
    "Si nous regardons à l'intérieur : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outs:\n",
      "- md5: 0cde6c4b0e77e95a1ff808ddc9da446a\n",
      "  size: 1161688\n",
      "  hash: md5\n",
      "  path: hour.csv\n"
     ]
    }
   ],
   "source": [
    "!type data\\hour.csv.dvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que le fichier contient des informations à propos de notre csv :  \n",
    "- Un hash du fichier \n",
    "- l'algorithme de hashage utilisé\n",
    "- la taille\n",
    "- le chemin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un gitignore a été créé par défaut, si on regarde à l'intérieur : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hour.csv\n"
     ]
    }
   ],
   "source": [
    "!type data\\.gitignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir que notre csv y est renseigné."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant ajoutons le nouveau fichier data/hour.csv.dvc et le fichier data/.gitignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add data/.gitignore data/hour.csv.dvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "et commitons le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master aed3bce] add .dvc file to track hours.csv file\n",
      " 2 files changed, 6 insertions(+)\n",
      " create mode 100644 data/.gitignore\n",
      " create mode 100644 data/hour.csv.dvc\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"add .dvc file to track hours.csv file\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une bonne idée est de créer un tag pour chaque version de notre dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git tag -a v1 -m \"raw data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre donnée est toujours sur notre dossier en local, maintenant nous devons l'envoyer sur notre stokage distant (qui pour rappel et en fait un autre dossier local). Pour ça nous utilisons la commande dvc push."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 file pushed\n"
     ]
    }
   ],
   "source": [
    "!dvc push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut regarder dans notre \"remote storage\" ce que nous avons :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C s'appelle OS\n",
      " Le num�ro de s�rie du volume est 506E-5B85\n",
      "\n",
      " R�pertoire de c:\\tmp\\dvc-storage\n",
      "\n",
      "15/11/2023  11:08    <DIR>          .\n",
      "15/11/2023  11:08    <DIR>          ..\n",
      "15/11/2023  11:08    <DIR>          files\n",
      "               0 fichier(s)                0 octets\n",
      "\n",
      " R�pertoire de c:\\tmp\\dvc-storage\\files\n",
      "\n",
      "15/11/2023  11:08    <DIR>          .\n",
      "15/11/2023  11:08    <DIR>          ..\n",
      "15/11/2023  11:08    <DIR>          md5\n",
      "               0 fichier(s)                0 octets\n",
      "\n",
      " R�pertoire de c:\\tmp\\dvc-storage\\files\\md5\n",
      "\n",
      "15/11/2023  11:08    <DIR>          .\n",
      "15/11/2023  11:08    <DIR>          ..\n",
      "15/11/2023  11:08    <DIR>          0c\n",
      "               0 fichier(s)                0 octets\n",
      "\n",
      " R�pertoire de c:\\tmp\\dvc-storage\\files\\md5\\0c\n",
      "\n",
      "15/11/2023  11:08    <DIR>          .\n",
      "15/11/2023  11:08    <DIR>          ..\n",
      "15/11/2023  11:08         1�161�688 de6c4b0e77e95a1ff808ddc9da446a\n",
      "               1 fichier(s)        1�161�688 octets\n",
      "\n",
      "     Total des fichiers list�s�:\n",
      "               1 fichier(s)        1�161�688 octets\n",
      "              11 R�p(s)  340�679�917�568 octets libres\n"
     ]
    }
   ],
   "source": [
    "!dir /s \\tmp\\dvc-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons voir que notre fichier est présent dans le dossier mais avec un nom différent, ce nom correspond au hash de la donnée du fichier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nos données sont sauvegardées à distance, nous pouvons les supprimer localement. Sauf le fichier .dvc ! Car sinon vous perdrez le lien avec vos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C s'appelle OS\n",
      " Le num�ro de s�rie du volume est 506E-5B85\n",
      "\n",
      " R�pertoire de c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\data\n",
      "\n",
      "15/11/2023  11:06    <DIR>          .\n",
      "15/11/2023  11:05    <DIR>          ..\n",
      "15/11/2023  11:06                11 .gitignore\n",
      "15/11/2023  11:06         1�161�688 hour.csv\n",
      "15/11/2023  11:06                96 hour.csv.dvc\n",
      "               3 fichier(s)        1�161�795 octets\n",
      "               2 R�p(s)  340�681�932�800 octets libres\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "!dir data\n",
    "!rm -rf data/hour.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C s'appelle OS\n",
      " Le num�ro de s�rie du volume est 506E-5B85\n",
      "\n",
      " R�pertoire de c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\data\n",
      "\n",
      "15/11/2023  11:06    <DIR>          .\n",
      "15/11/2023  11:05    <DIR>          ..\n",
      "15/11/2023  11:06                11 .gitignore\n",
      "15/11/2023  11:06         1�161�688 hour.csv\n",
      "15/11/2023  11:06                96 hour.csv.dvc\n",
      "               3 fichier(s)        1�161�795 octets\n",
      "               2 R�p(s)  340�681�928�704 octets libres\n"
     ]
    }
   ],
   "source": [
    "!dir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un autre emplacement où vos données résident est le dossier .dvc/cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Le fichier sp�cifi� est introuvable.\n",
      "Le chemin d'acc�s sp�cifi� est introuvable.\n"
     ]
    }
   ],
   "source": [
    "!dir .dvc\\cache\\files\\md5\\0a\\1c63297d478edfdcc18433bb509cd5\n",
    "!type .dvc\\cache\\files\\md5\\0a\\1c63297d478edfdcc18433bb509cd5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous supprimons aussi les données à l'intérieur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rd /s /q .dvc\\cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nous voulons récupérer nos données localement, nous pouvons utiliser dvc pull pour récupérer les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 file fetched\n"
     ]
    }
   ],
   "source": [
    "!dvc pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nous regardons dans le dossier data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C s'appelle OS\n",
      " Le num�ro de s�rie du volume est 506E-5B85\n",
      "\n",
      " R�pertoire de c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\data\n",
      "\n",
      "15/11/2023  11:06    <DIR>          .\n",
      "15/11/2023  11:05    <DIR>          ..\n",
      "15/11/2023  11:06                11 .gitignore\n",
      "15/11/2023  11:06         1�161�688 hour.csv\n",
      "15/11/2023  11:06                96 hour.csv.dvc\n",
      "               3 fichier(s)        1�161�795 octets\n",
      "               2 R�p(s)  340�682�764�288 octets libres\n"
     ]
    }
   ],
   "source": [
    "!dir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre fichier est de retour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant modifions nos données !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C s'appelle OS\n",
      " Le num�ro de s�rie du volume est 506E-5B85\n",
      "\n",
      " R�pertoire de c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\data\n",
      "\n",
      "15/11/2023  11:06         1�161�688 hour.csv\n",
      "               1 fichier(s)        1�161�688 octets\n",
      "               0 R�p(s)  340�682�764�288 octets libres\n"
     ]
    }
   ],
   "source": [
    "!dir data\\hour.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sed -i '2,1001d' data/hour.csv\n",
    "!powershell -Command \"(Get-Content data\\hour.csv | Select-Object -Skip 1000) | Set-Content data\\hour.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C s'appelle OS\n",
      " Le num�ro de s�rie du volume est 506E-5B85\n",
      "\n",
      " R�pertoire de c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\data\n",
      "\n",
      "15/11/2023  11:11         1�098�095 hour.csv\n",
      "               1 fichier(s)        1�098�095 octets\n",
      "               0 R�p(s)  340�682�588�160 octets libres\n"
     ]
    }
   ],
   "source": [
    "!dir data\\hour.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et répétons les opérations précédantes : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⠋ Checking graph\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add 'data\\hour.csv.dvc'\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n"
     ]
    }
   ],
   "source": [
    "!dvc add data/hour.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add data/hour.csv.dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: pathspec 'remove' did not match any file(s) known to git\n",
      "error: pathspec '1000' did not match any file(s) known to git\n",
      "error: pathspec 'lines'' did not match any file(s) known to git\n"
     ]
    }
   ],
   "source": [
    "!git commit -m 'data: remove 1000 lines'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git tag -a v2 -m \"removed 1000 lines\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 file pushed\n"
     ]
    }
   ],
   "source": [
    "!dvc push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Le chemin d'acc�s sp�cifi� est introuvable.\n"
     ]
    }
   ],
   "source": [
    "!rd /s /q .data\\hour.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rd /s /q .dvc\\cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant regarder dans notre git log, et voir l'historique des modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commit aed3bcef3ae76d32855bf41ebd6bb50dbe8420ce\n",
      "Author: zacharyak30 <zachary.akakpo@efrei.net>\n",
      "Date:   Wed Nov 15 11:07:43 2023 +0100\n",
      "\n",
      "    add .dvc file to track hours.csv file\n",
      "\n",
      "commit 819acdbfcbbb1cdae256de97aa9efe0a064cc872\n",
      "Author: zacharyak30 <zachary.akakpo@efrei.net>\n",
      "Date:   Wed Nov 15 11:05:50 2023 +0100\n",
      "\n",
      "    add remote storage\n",
      "\n",
      "commit 1d98e7badf3707887ee1e7fc6ce6d96ebe6a1a2e\n",
      "Author: zacharyak30 <zachary.akakpo@efrei.net>\n",
      "Date:   Wed Nov 15 11:04:50 2023 +0100\n",
      "\n",
      "    initialize repo\n",
      "\n",
      "commit ce791de11881aa46b8441aea859084585b02b952\n",
      "Author: zacharyak30 <zachary.akakpo@efrei.net>\n",
      "Date:   Wed Nov 15 11:01:48 2023 +0100\n",
      "\n",
      "    starting\n"
     ]
    }
   ],
   "source": [
    "!git log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour accéder et extraire des versions spécifiques de nos données nous pouvons utiliser le package dvc en python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dvc==3.29.0\n",
      "dvc-data==2.20.0\n",
      "dvc-http==2.30.2\n",
      "dvc-objects==1.2.0\n",
      "dvc-render==0.6.0\n",
      "dvc-studio-client==0.15.0\n",
      "dvc-task==0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | findstr dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dvc\n",
    "import dvc.api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "RevError",
     "evalue": "unknown Git revision 'v1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRevError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\dvc\\scm.py:172\u001b[0m, in \u001b[0;36mresolve_rev\u001b[1;34m(scm, rev)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m     \u001b[39mreturn\u001b[39;00m scm\u001b[39m.\u001b[39;49mresolve_rev(fix_exp_head(scm, rev))\n\u001b[0;32m    173\u001b[0m \u001b[39mexcept\u001b[39;00m InternalRevError \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\scmrepo\\git\\__init__.py:292\u001b[0m, in \u001b[0;36mGit._backend_func\u001b[1;34m(self, name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    291\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(backend, name)\n\u001b[1;32m--> 292\u001b[0m result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    293\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_backend \u001b[39m=\u001b[39m key\n",
      "File \u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\scmrepo\\git\\backend\\pygit2\\__init__.py:392\u001b[0m, in \u001b[0;36mPygit2Backend.resolve_rev\u001b[1;34m(self, rev)\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[39mreturn\u001b[39;00m shas\u001b[39m.\u001b[39mpop()  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m--> 392\u001b[0m \u001b[39mraise\u001b[39;00m RevError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munknown Git revision \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrev\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mRevError\u001b[0m: unknown Git revision 'v1'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRevError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\Mlflow_tp_efrei_etudiant.ipynb Cell 112\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zacha/Documents/CourS9/ADDE92Applications%20of%20Big%20Data/TP1/Mlflow_tp_efrei_etudiant.ipynb#Y216sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m repo \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/ZacharyAk30/TP1.git\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# The URL of your DVC repository\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zacha/Documents/CourS9/ADDE92Applications%20of%20Big%20Data/TP1/Mlflow_tp_efrei_etudiant.ipynb#Y216sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m version \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mv1\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# The version of your dataset\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zacha/Documents/CourS9/ADDE92Applications%20of%20Big%20Data/TP1/Mlflow_tp_efrei_etudiant.ipynb#Y216sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m data_url \u001b[39m=\u001b[39m dvc\u001b[39m.\u001b[39;49mapi\u001b[39m.\u001b[39;49mget_url(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zacha/Documents/CourS9/ADDE92Applications%20of%20Big%20Data/TP1/Mlflow_tp_efrei_etudiant.ipynb#Y216sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     path\u001b[39m=\u001b[39;49mpath, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zacha/Documents/CourS9/ADDE92Applications%20of%20Big%20Data/TP1/Mlflow_tp_efrei_etudiant.ipynb#Y216sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     repo\u001b[39m=\u001b[39;49mrepo,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zacha/Documents/CourS9/ADDE92Applications%20of%20Big%20Data/TP1/Mlflow_tp_efrei_etudiant.ipynb#Y216sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     rev\u001b[39m=\u001b[39;49mversion\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zacha/Documents/CourS9/ADDE92Applications%20of%20Big%20Data/TP1/Mlflow_tp_efrei_etudiant.ipynb#Y216sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\dvc\\api\\data.py:45\u001b[0m, in \u001b[0;36mget_url\u001b[1;34m(path, repo, rev, remote)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mif\u001b[39;00m remote:\n\u001b[0;32m     44\u001b[0m     repo_kwargs[\u001b[39m\"\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mcore\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m\"\u001b[39m\u001b[39mremote\u001b[39m\u001b[39m\"\u001b[39m: remote}}\n\u001b[1;32m---> 45\u001b[0m \u001b[39mwith\u001b[39;00m Repo\u001b[39m.\u001b[39;49mopen(\n\u001b[0;32m     46\u001b[0m     repo, rev\u001b[39m=\u001b[39;49mrev, subrepos\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, uninitialized\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrepo_kwargs\n\u001b[0;32m     47\u001b[0m ) \u001b[39mas\u001b[39;00m _repo:\n\u001b[0;32m     48\u001b[0m     index, entry \u001b[39m=\u001b[39m _repo\u001b[39m.\u001b[39mget_data_index_entry(path)\n\u001b[0;32m     49\u001b[0m     \u001b[39mwith\u001b[39;00m reraise(\n\u001b[0;32m     50\u001b[0m         (StorageKeyError, \u001b[39mValueError\u001b[39;00m),\n\u001b[0;32m     51\u001b[0m         NoRemoteError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mno remote specified in \u001b[39m\u001b[39m{\u001b[39;00m_repo\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     52\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\dvc\\repo\\__init__.py:304\u001b[0m, in \u001b[0;36mRepo.open\u001b[1;34m(url, *args, **kwargs)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    301\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopen\u001b[39m(url: Optional[\u001b[39mstr\u001b[39m], \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRepo\u001b[39m\u001b[39m\"\u001b[39m:  \u001b[39m# noqa: A003\u001b[39;00m\n\u001b[0;32m    302\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mopen_repo\u001b[39;00m \u001b[39mimport\u001b[39;00m open_repo\n\u001b[1;32m--> 304\u001b[0m     \u001b[39mreturn\u001b[39;00m open_repo(url, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\dvc\\repo\\open_repo.py:64\u001b[0m, in \u001b[0;36mopen_repo\u001b[1;34m(url, *args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[39mexcept\u001b[39;00m NotDvcRepoError:\n\u001b[0;32m     62\u001b[0m         \u001b[39mpass\u001b[39;00m  \u001b[39m# fallthrough to _external_repo\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m \u001b[39mreturn\u001b[39;00m _external_repo(url, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\ApplicationDev\\miniconda3\\Lib\\contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[0;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[0;32m     80\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 81\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\dvc\\repo\\open_repo.py:47\u001b[0m, in \u001b[0;36m_external_repo\u001b[1;34m(url, rev, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m main_root \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     38\u001b[0m repo_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[0;32m     39\u001b[0m     root_dir\u001b[39m=\u001b[39mpath,\n\u001b[0;32m     40\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m     45\u001b[0m )\n\u001b[1;32m---> 47\u001b[0m \u001b[39mreturn\u001b[39;00m Repo(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrepo_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\dvc\\repo\\__init__.py:179\u001b[0m, in \u001b[0;36mRepo.__init__\u001b[1;34m(self, root_dir, fs, rev, subrepos, uninitialized, config, url, repo_factory, scm, remote, remote_config)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_dir: \u001b[39mstr\u001b[39m\n\u001b[0;32m    175\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdvc_dir: Optional[\u001b[39mstr\u001b[39m]\n\u001b[0;32m    176\u001b[0m (\n\u001b[0;32m    177\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_dir,\n\u001b[0;32m    178\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdvc_dir,\n\u001b[1;32m--> 179\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_repo_dirs(\n\u001b[0;32m    180\u001b[0m     root_dir\u001b[39m=\u001b[39;49mroot_dir,\n\u001b[0;32m    181\u001b[0m     fs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfs,\n\u001b[0;32m    182\u001b[0m     uninitialized\u001b[39m=\u001b[39;49muninitialized,\n\u001b[0;32m    183\u001b[0m     scm\u001b[39m=\u001b[39;49mscm,\n\u001b[0;32m    184\u001b[0m )\n\u001b[0;32m    186\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_uninitialized \u001b[39m=\u001b[39m uninitialized\n\u001b[0;32m    188\u001b[0m \u001b[39m# used by DVCFileSystem to determine if it should traverse subrepos\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\dvc\\repo\\__init__.py:112\u001b[0m, in \u001b[0;36mRepo._get_repo_dirs\u001b[1;34m(self, root_dir, fs, uninitialized, scm)\u001b[0m\n\u001b[0;32m    110\u001b[0m dvc_dir: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 112\u001b[0m     root_dir \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind_root(root_dir, fs)\n\u001b[0;32m    113\u001b[0m     fs \u001b[39m=\u001b[39m fs \u001b[39mor\u001b[39;00m localfs\n\u001b[0;32m    114\u001b[0m     dvc_dir \u001b[39m=\u001b[39m fs\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(root_dir, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDVC_DIR)\n",
      "File \u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\dvc\\repo\\__init__.py:411\u001b[0m, in \u001b[0;36mRepo.find_root\u001b[1;34m(cls, root, fs)\u001b[0m\n\u001b[0;32m    409\u001b[0m fs \u001b[39m=\u001b[39m fs \u001b[39mor\u001b[39;00m localfs\n\u001b[0;32m    410\u001b[0m root \u001b[39m=\u001b[39m root \u001b[39mor\u001b[39;00m os\u001b[39m.\u001b[39mcurdir\n\u001b[1;32m--> 411\u001b[0m root_dir \u001b[39m=\u001b[39m fs\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39mabspath(root)\n\u001b[0;32m    413\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fs\u001b[39m.\u001b[39misdir(root_dir):\n\u001b[0;32m    414\u001b[0m     \u001b[39mraise\u001b[39;00m NotDvcRepoError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdirectory \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mroot\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m does not exist\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\ApplicationDev\\miniconda3\\Lib\\functools.py:1001\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m    999\u001b[0m val \u001b[39m=\u001b[39m cache\u001b[39m.\u001b[39mget(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[0;32m   1000\u001b[0m \u001b[39mif\u001b[39;00m val \u001b[39mis\u001b[39;00m _NOT_FOUND:\n\u001b[1;32m-> 1001\u001b[0m     val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(instance)\n\u001b[0;32m   1002\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1003\u001b[0m         cache[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattrname] \u001b[39m=\u001b[39m val\n",
      "File \u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\dvc\\fs\\git.py:51\u001b[0m, in \u001b[0;36mGitFileSystem.path\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mcached_property\n\u001b[0;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpath\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfs\u001b[39m.\u001b[39mpath\n",
      "File \u001b[1;32m~\\ApplicationDev\\miniconda3\\Lib\\functools.py:1001\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m    999\u001b[0m val \u001b[39m=\u001b[39m cache\u001b[39m.\u001b[39mget(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[0;32m   1000\u001b[0m \u001b[39mif\u001b[39;00m val \u001b[39mis\u001b[39;00m _NOT_FOUND:\n\u001b[1;32m-> 1001\u001b[0m     val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(instance)\n\u001b[0;32m   1002\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1003\u001b[0m         cache[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattrname] \u001b[39m=\u001b[39m val\n",
      "File \u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\dvc\\fs\\git.py:47\u001b[0m, in \u001b[0;36mGitFileSystem.fs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mcached_property\n\u001b[0;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfs\u001b[39m(\n\u001b[0;32m     43\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     44\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mFsspecGitFileSystem\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     45\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mscmrepo\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfs\u001b[39;00m \u001b[39mimport\u001b[39;00m GitFileSystem \u001b[39mas\u001b[39;00m FsspecGitFileSystem\n\u001b[1;32m---> 47\u001b[0m     \u001b[39mreturn\u001b[39;00m FsspecGitFileSystem(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfs_args)\n",
      "File \u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\fsspec\\spec.py:79\u001b[0m, in \u001b[0;36m_Cached.__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_cache[token]\n\u001b[0;32m     78\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 79\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     80\u001b[0m     \u001b[39m# Setting _fs_token here causes some static linters to complain.\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     obj\u001b[39m.\u001b[39m_fs_token_ \u001b[39m=\u001b[39m token\n",
      "File \u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\scmrepo\\fs.py:165\u001b[0m, in \u001b[0;36mGitFileSystem.__init__\u001b[1;34m(self, path, rev, scm, trie, rev_resolver, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m scm \u001b[39m=\u001b[39m scm \u001b[39mor\u001b[39;00m Git(path)\n\u001b[0;32m    164\u001b[0m resolver \u001b[39m=\u001b[39m rev_resolver \u001b[39mor\u001b[39;00m Git\u001b[39m.\u001b[39mresolve_rev\n\u001b[1;32m--> 165\u001b[0m resolved \u001b[39m=\u001b[39m resolver(scm, rev \u001b[39mor\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    166\u001b[0m tree_obj \u001b[39m=\u001b[39m scm\u001b[39m.\u001b[39mpygit2\u001b[39m.\u001b[39mget_tree_obj(rev\u001b[39m=\u001b[39mresolved)\n\u001b[0;32m    167\u001b[0m trie \u001b[39m=\u001b[39m GitTrie(tree_obj, resolved)\n",
      "File \u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\dvc\\scm.py:187\u001b[0m, in \u001b[0;36mresolve_rev\u001b[1;34m(scm, rev)\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[39mif\u001b[39;00m ref_infos:\n\u001b[0;32m    185\u001b[0m         \u001b[39mreturn\u001b[39;00m scm\u001b[39m.\u001b[39mget_ref(\u001b[39mstr\u001b[39m(ref_infos))\n\u001b[1;32m--> 187\u001b[0m \u001b[39mraise\u001b[39;00m RevError(\u001b[39mstr\u001b[39m(exc))\n",
      "\u001b[1;31mRevError\u001b[0m: unknown Git revision 'v1'"
     ]
    }
   ],
   "source": [
    "path = \"data/hour.csv\"  # The path to the file in your DVC repository\n",
    "repo = \"https://github.com/ZacharyAk30/TP1.git\"  # The URL of your DVC repository\n",
    "version = \"v1\"  # The version of your dataset\n",
    "\n",
    "data_url = dvc.api.get_url(\n",
    "    path=path, \n",
    "    repo=repo,\n",
    "    rev=version\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:5000\n"
     ]
    }
   ],
   "source": [
    "MLFLOW_TRACKING_URI = \"http://localhost:5000\"\n",
    "print(MLFLOW_TRACKING_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment created. ID: 188222493590758486\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "config_exp(name=\"ML_EXP_WITH_DVC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\Mlflow_tp_efrei_etudiant.ipynb Cell 115\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zacha/Documents/CourS9/ADDE92Applications%20of%20Big%20Data/TP1/Mlflow_tp_efrei_etudiant.ipynb#Y222sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zacha/Documents/CourS9/ADDE92Applications%20of%20Big%20Data/TP1/Mlflow_tp_efrei_etudiant.ipynb#Y222sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(data_url, sep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zacha/Documents/CourS9/ADDE92Applications%20of%20Big%20Data/TP1/Mlflow_tp_efrei_etudiant.ipynb#Y222sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(data\u001b[39m.\u001b[39mhead())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_url' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(data_url, sep=\",\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\Mlflow_tp_efrei_etudiant.ipynb Cell 116\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zacha/Documents/CourS9/ADDE92Applications%20of%20Big%20Data/TP1/Mlflow_tp_efrei_etudiant.ipynb#Y223sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatetime\u001b[39;00m \u001b[39mimport\u001b[39;00m datetime, time\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zacha/Documents/CourS9/ADDE92Applications%20of%20Big%20Data/TP1/Mlflow_tp_efrei_etudiant.ipynb#Y223sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m data\u001b[39m.\u001b[39mindex \u001b[39m=\u001b[39m raw_data\u001b[39m.\u001b[39mapply(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zacha/Documents/CourS9/ADDE92Applications%20of%20Big%20Data/TP1/Mlflow_tp_efrei_etudiant.ipynb#Y223sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mlambda\u001b[39;00m row: datetime\u001b[39m.\u001b[39mcombine(row\u001b[39m.\u001b[39mname, time(hour\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(row[\u001b[39m'\u001b[39m\u001b[39mhr\u001b[39m\u001b[39m'\u001b[39m]))), axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, time\n",
    "\n",
    "data.index = raw_data.apply(\n",
    "    lambda row: datetime.combine(row.name, time(hour=int(row['hr']))), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant utiliser 2 mécanismes pour ajouter plus d'informations sur notre jeu de données dans MLFlow :    \n",
    "-Grâce à dvc, nous avons maintenant des liens vers les différentes versions de notre jeu de données.   \n",
    "-Nous pouvons l'utiliser en combinaison avec le module mlflow.data pour ajouter plus d'informations sur notre jeu de données.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'cnt'\n",
    "prediction = 'prediction'\n",
    "numerical_features = ['temp', 'atemp', 'hum', 'windspeed', 'hr', 'weekday']\n",
    "categorical_features = ['season', 'holiday', 'workingday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\Mlflow_tp_efrei_etudiant.ipynb Cell 120\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zacha/Documents/CourS9/ADDE92Applications%20of%20Big%20Data/TP1/Mlflow_tp_efrei_etudiant.ipynb#Y230sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m start_date \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m2011-01-01 00:00:00\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zacha/Documents/CourS9/ADDE92Applications%20of%20Big%20Data/TP1/Mlflow_tp_efrei_etudiant.ipynb#Y230sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m end_date \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m2011-01-28 23:00:00\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zacha/Documents/CourS9/ADDE92Applications%20of%20Big%20Data/TP1/Mlflow_tp_efrei_etudiant.ipynb#Y230sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dataset \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mloc[start_date:end_date]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "start_date = '2011-01-01 00:00:00'\n",
    "end_date = '2011-01-28 23:00:00'\n",
    "dataset = data.loc[start_date:end_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la même cellule : \n",
    "\n",
    "1)\n",
    "- Aller voir la documentation du module mlflow data et importer le bon objet pour les données pandas.  \n",
    "- Créer un run avec la date, l'heure, etc. comme nom.    \n",
    "- pour l'entrainement vous pouvez utiliser le chemin vers votre dataset lors de la création de votre dataframe.   \n",
    "\n",
    "2) \n",
    "- logger le dataset avec la méthode appropriée.  \n",
    "- logger également le chemin vers votre dataset.   \n",
    "- logger la version du dataset utilisée.   \n",
    "\n",
    "3)\n",
    "- Créer un fichier texte et logger le en tant qu'artifact. Dans ce fichier vous pourrez indiquer la colonne qui a servi de target, les features numériques et les features catégorielles.\n",
    "\n",
    "4)\n",
    "- N'oubliez pas d'utiliser la fonction mlflow.end_run() si vous n'avez pas utilisez de with pour le run.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# 1)\n",
    "from mlflow.tracking import MlflowClient\n",
    "client = MlflowClient()\n",
    "\n",
    "# Create a run with date, time as name\n",
    "run_name = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "run = client.create_run(\"0\", run_name=run_name)\n",
    "\n",
    "# Load your dataset\n",
    "data_path = \"path_to_your_dataset\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# 2)\n",
    "# Log the dataset\n",
    "mlflow.log_param(\"data_path\", data_path)\n",
    "mlflow.log_param(\"data_version\", \"v1\")  # replace with your data version\n",
    "\n",
    "# 3)\n",
    "# Create a text file and log it as an artifact\n",
    "with open(\"info.txt\", \"w\") as f:\n",
    "    f.write(\"Target column: target\\n\")  # replace with your target column\n",
    "    f.write(\"Numerical features: feature1, feature2\\n\")  # replace with your numerical features\n",
    "    f.write(\"Categorical features: feature3, feature4\\n\")  # replace with your categorical features\n",
    "\n",
    "mlflow.log_artifact(\"info.txt\")\n",
    "\n",
    "# 4)\n",
    "# End the run\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Déploiement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.Trouver comment transitionner un modèle en état staging et ensuite dans l'état production. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# Transition model version 1 of model 'my_model' to 'Staging'\n",
    "client.transition_model_version_stage(\n",
    "  name=\"my_model\",\n",
    "  version=1,\n",
    "  stage=\"Staging\",\n",
    ")\n",
    "\n",
    "# Later, transition the same model version to 'Production'\n",
    "client.transition_model_version_stage(\n",
    "  name=\"my_model\",\n",
    "  version=1,\n",
    "  stage=\"Production\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.Créer une fonction qui récupère la version du modèle avec les meilleurs metrics d'entraînement et qui transitionne ce modèle dans l'état production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "def promote_best_model_to_production(model_name, metric_name):\n",
    "    client = MlflowClient()\n",
    "\n",
    "    # Get a list of all versions of the model\n",
    "    versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "\n",
    "    # Find the version with the best metric\n",
    "    best_version = None\n",
    "    best_metric = float('inf')\n",
    "    for version in versions:\n",
    "        run_id = version.run_id\n",
    "        run = client.get_run(run_id)\n",
    "        if metric_name in run.data.metrics:\n",
    "            metric_value = run.data.metrics[metric_name]\n",
    "            if metric_value < best_metric:\n",
    "                best_metric = metric_value\n",
    "                best_version = version.version\n",
    "\n",
    "    if best_version is not None:\n",
    "        # Transition the best version to 'Production'\n",
    "        client.transition_model_version_stage(\n",
    "            name=model_name,\n",
    "            version=best_version,\n",
    "            stage=\"Production\",\n",
    "        )\n",
    "        print(f\"Promoted version {best_version} of model {model_name} to 'Production'\")\n",
    "    else:\n",
    "        print(f\"No suitable version found for model {model_name}\")\n",
    "\n",
    "# Usage:\n",
    "promote_best_model_to_production(\"my_model\", \"rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Le serveur mlflow peut vous fournir des prédictions à partir des modèles enregistrés. Faites en sorte d'obtenir une prédiction de votre dernier modèle en requêtant le serveur mlflow. (Voir : https://mlflow.org/docs/latest/models.html#command-line-interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "\n",
    "# The path to the model in the MLflow server\n",
    "model_uri = \"runs:/<run_id>/model\"\n",
    "\n",
    "# Load the model\n",
    "model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# Create a sample input vector\n",
    "input_vector = pd.DataFrame([1, 2, 3, 4, 5])\n",
    "\n",
    "# Predict the output for the input vector\n",
    "output_vector = model.predict(input_vector)\n",
    "\n",
    "print(output_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.Créer un script à part qui pull le dernier modèle depuis le model registry. Plus tard vous pourrez utiliser ce script pour récupérer le modèle dans une api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "def get_latest_model(model_name):\n",
    "    client = MlflowClient()\n",
    "    model_version_details = client.get_latest_versions(model_name, stages=[\"Production\"])\n",
    "    if model_version_details:\n",
    "        latest_version = model_version_details[0].version\n",
    "        model_uri = f\"models:/{model_name}/{latest_version}\"\n",
    "        model = mlflow.pyfunc.load_model(model_uri)\n",
    "        return model\n",
    "    else:\n",
    "        print(f\"No production model found for {model_name}\")\n",
    "        return None\n",
    "\n",
    "# Usage:\n",
    "model = get_latest_model(\"my_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Entraînement d'un CNN et log des metrics dans MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir le notebook donnée par le formateur."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow_tp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
