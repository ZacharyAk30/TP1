{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP MLFLOW\n",
    "Florent Jakubowski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Prise en main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Installez le package mlflow avec python dans un environnement virtuel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Dans un terminal lancez un serveur mlflow. Aller voir dans votre navigateur, sur le port correspondant, l'ui de mlflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mlflow server --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow server --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque vous lancez mflow sans option par défaut mlflow va stocker toute la donnée dont il a besoin sur votre file system. Vous aurez notamment un dossier mlruns qui se créra par défaut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Dans un notebook, utilisez le package mlflow pour vous connecter au serveur mlflow que vous avez lancé. Utilisez la bonne fonction pour paramétrer l'adresse du serveur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:149: UserWarning: Field \"model_server_url\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\pydantic\\_internal\\_config.py:318: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention ! A chaque fois que vous effectuerez une opération sur mlflow dans une autre cellule de votre notebook vous devrez vérifier avant que vous pointez bien sur le bon serveur mlflow. Il existe aussi une fonction pour connaître quelle adresse de serveur a été enregistrée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://localhost:5000'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créez une experiment via votre notebook ou avec via l'ui. \n",
    "Une experiment, ou une expérience en français, est un ensemble de run que vous avez effectué. Le but est de trouver in fine les meilleurs paramètres, modèles ou hyperparamètres pour votre besoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment already exists\n"
     ]
    }
   ],
   "source": [
    "try : \n",
    "    experiment_id = mlflow.create_experiment(\"My Experiment\")\n",
    "    print(\"Experiment created with ID: \", experiment_id)\n",
    "except:\n",
    "    print(\"Experiment already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pareillement à l'adresse du serveur mlflow à chaque fois que vous exécuterez un run dans une cellule vous devrez définir l'experiment sur laquelle vous voulez envoyer votre run. Cherchez dans la documentation la fonction permettant de faire cela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/636497380083101659', creation_time=1700040769157, experiment_id='636497380083101659', last_update_time=1700040769157, lifecycle_stage='active', name='My Experiment', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"My Experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous vous proposons de créer une fonction `configure_experiment` permettant de créer une expérience ou de définir l'expérience si elle existe déjà."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_exp(name):\n",
    "    # Check if the experiment exists\n",
    "    experiment = mlflow.get_experiment_by_name(name)\n",
    "    \n",
    "    if experiment is None:\n",
    "        # If the experiment does not exist, create it\n",
    "        experiment_id = mlflow.create_experiment(name)\n",
    "        print(f\"Experiment created. ID: {experiment_id}\")\n",
    "    else:\n",
    "        # If the experiment exists, set it\n",
    "        mlflow.set_experiment(name)\n",
    "        print(f\"Experiment set to {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Après avoir créé l'experiment. Nous allons entraîner notre modèle. Prenez le dataset wine de la librairie sklearn et utilisez un algorithme de la famille des arbres de décisions. Nous allons lors de l'entraînement de notre modèle logger les mesures.   \n",
    "\n",
    "Pour cela nous voulons lancer un nouveau run dans notre experiment sur mlflow. Le run correspond à un entraînement du modèle.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Créer une variable run_name pour le nouveau run que vous voulez créer avec la date du jour, l'heure, la minute et la seconde dans le nom.   \n",
    "-Trouvez comment logger les paramètres d'entraînement, les hyperparamètres et les performances du modèle (metrics) explicitement dans mlflow en lançant votre run de manière manuelle.   \n",
    "-Ajoutez également votre modèle avec un nom distinctif grâce à la méthode adéquat, vous devrez réutiliser ce nom lors des prochains entraînements. Que voyez-vous dans l'ui de mlflow ?   \n",
    "\n",
    "\n",
    "Vous pouvez utiliser `with` pour ne pas avoir besoin d'utiliser la fonction `end_run()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained and logged with run name:  20231115-132557\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the wine dataset\n",
    "wine = datasets.load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a decision tree classifier\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Train the classifier\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set results\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Create a run name with the current date and time\n",
    "run_name = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Start a new run\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "    # Log the model parameters and metrics\n",
    "    mlflow.log_param(\"criterion\", clf.criterion)\n",
    "    mlflow.log_param(\"splitter\", clf.splitter)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    # Save the model with a distinctive name\n",
    "    mlflow.sklearn.log_model(clf, \"model\")\n",
    "\n",
    "print(\"Model trained and logged with run name: \", run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut utiliser `mlflow.active_run()` pour être sûr que le run est bien terminé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.active_run() is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ecrivez un run sans utiliser un with et sans utilisez mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run started with run_id:  1666b06284ba4213aa9e705f81eefb73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "run = mlflow.start_run(run_name=\"My Run\")\n",
    "mlflow.log_param(\"param1\", \"value1\")\n",
    "mlflow.log_metric(\"metric1\", 1.23)\n",
    "mlflow.sklearn.log_model(clf, \"model\")\n",
    "\n",
    "print(\"Run started with run_id: \", run.info.run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `active_run()` nous retourne normalement ce run ci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ActiveRun: >\n"
     ]
    }
   ],
   "source": [
    "run = mlflow.active_run()\n",
    "print(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faisons bien attention de bien le fermer pour ne pas avoir de comportements exotiques ensuite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisez mlflow.end_run() et vérifiez avec active_run() qu'aucun run n'est retourné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.end_run()\n",
    "mlflow.active_run() is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe une autre manière de logger automatiquement des paramètres et des metrics à vous de la trouver.   \n",
    "(Attention une fois activée cette fonction entrainera toujours un log automatique, veillez à la désactiver pour la suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/15 13:26:15 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\"\n",
      "2023/11/15 13:26:15 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\"\n",
      "2023/11/15 13:26:15 WARNING mlflow.tracking.fluent: Exception raised while enabling autologging for sklearn: C:\\Users\\zacha\\ApplicationDev\\miniconda3\\Lib\\distutils\\core.py\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "mlflow.autolog(log_models=False, exclusive=True)\n",
    "# mlflow.sklearn.autolog(log_models=True)\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Changez un ou plusieurs hyperparamètres et relancez un entraînement avec le même nom de modèle. Rendez-vous dans l'onglet model de mlflow que constatez-vous ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "# Create a decision tree classifier with different hyperparameters\n",
    "clf = tree.DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "# Train the classifier\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# Log the model\n",
    "with mlflow.start_run(run_name=\"My Run\"):\n",
    "    mlflow.sklearn.log_model(clf, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez récupérer le dernier run avec la fonction last_active_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Run: data=<RunData: metrics={}, params={}, tags={'mlflow.log-model.history': '[{\"run_id\": \"c241adf84be1402d800245ac18cece2e\", '\n",
      "                             '\"artifact_path\": \"model\", \"utc_time_created\": '\n",
      "                             '\"2023-11-15 12:26:19.059562\", \"flavors\": '\n",
      "                             '{\"python_function\": {\"model_path\": \"model.pkl\", '\n",
      "                             '\"predict_fn\": \"predict\", \"loader_module\": '\n",
      "                             '\"mlflow.sklearn\", \"python_version\": \"3.11.5\", '\n",
      "                             '\"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": '\n",
      "                             '\"python_env.yaml\"}}, \"sklearn\": '\n",
      "                             '{\"pickled_model\": \"model.pkl\", '\n",
      "                             '\"sklearn_version\": \"1.3.2\", '\n",
      "                             '\"serialization_format\": \"cloudpickle\", \"code\": '\n",
      "                             'null}}, \"model_uuid\": '\n",
      "                             '\"0903b73df16244b8b577dc723d505e86\", '\n",
      "                             '\"mlflow_version\": \"2.8.0\", \"model_size_bytes\": '\n",
      "                             '2090}]',\n",
      " 'mlflow.runName': 'My Run',\n",
      " 'mlflow.source.name': 'c:\\\\Users\\\\zacha\\\\Documents\\\\CourS9\\\\ADDE92Applications '\n",
      "                       'of Big '\n",
      "                       'Data\\\\TP1\\\\envTP1\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py',\n",
      " 'mlflow.source.type': 'LOCAL',\n",
      " 'mlflow.user': 'zacha'}>, info=<RunInfo: artifact_uri='mlflow-artifacts:/636497380083101659/c241adf84be1402d800245ac18cece2e/artifacts', end_time=1700051181845, experiment_id='636497380083101659', lifecycle_stage='active', run_id='c241adf84be1402d800245ac18cece2e', run_name='My Run', run_uuid='c241adf84be1402d800245ac18cece2e', start_time=1700051179028, status='FINISHED', user_id='zacha'>, inputs=<RunInputs: dataset_inputs=[]>>\n"
     ]
    }
   ],
   "source": [
    "run = mlflow.last_active_run() \n",
    "print(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.Entraîner 3,4 modèles avec des hyperparamètres différents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# Define different sets of hyperparameters\n",
    "hyperparameters = [\n",
    "    {\"max_depth\": 3, \"min_samples_split\": 2},\n",
    "    {\"max_depth\": 5, \"min_samples_split\": 4},\n",
    "    {\"max_depth\": None, \"min_samples_split\": 2},\n",
    "    {\"max_depth\": 7, \"min_samples_split\": 10}\n",
    "]\n",
    "\n",
    "# Train a model for each set of hyperparameters\n",
    "for i, params in enumerate(hyperparameters):\n",
    "    # Create a decision tree classifier with the current hyperparameters\n",
    "    clf = tree.DecisionTreeClassifier(max_depth=params[\"max_depth\"], min_samples_split=params[\"min_samples_split\"])\n",
    "\n",
    "    # Train the classifier\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "\n",
    "    # Log the model\n",
    "    with mlflow.start_run(run_name=f\"My Run {i+1}\"):\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.sklearn.log_model(clf, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Versionning des données avec DVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque run le dataset utilisé est précisé dans mlflow. Mais les informations fournies sont assez pauvres. Pour améliorer ça nous allons utiliser l'outil dvc en combinaison avec mlflow.\n",
    "dvc fonctionne de pair avec git."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisez un repo git localement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized empty Git repository in C:/Users/zacha/Documents/CourS9/ADDE92Applications of Big Data/TP1/.git/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hint: Using 'master' as the name for the initial branch. This default branch name\n",
      "hint: is subject to change. To configure the initial branch name to use in all\n",
      "hint: of your new repositories, which will suppress this warning, call:\n",
      "hint: \n",
      "hint: \tgit config --global init.defaultBranch <name>\n",
      "hint: \n",
      "hint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\n",
      "hint: 'development'. The just-created branch can be renamed via this command:\n",
      "hint: \n",
      "hint: \tgit branch -m <name>\n"
     ]
    }
   ],
   "source": [
    "!git init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installez dvc avec pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dvc\n",
      "  Obtaining dependency information for dvc from https://files.pythonhosted.org/packages/e4/2b/2e04e73dbd9f87f5f4b3427e5053fe6c91f87e04c9028d704d91160f018a/dvc-3.29.0-py3-none-any.whl.metadata\n",
      "  Downloading dvc-3.29.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: colorama>=0.3.9 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from dvc) (0.4.6)\n",
      "Collecting configobj>=5.0.6 (from dvc)\n",
      "  Downloading configobj-5.0.8-py2.py3-none-any.whl (36 kB)\n",
      "Collecting distro>=1.3 (from dvc)\n",
      "  Using cached distro-1.8.0-py3-none-any.whl (20 kB)\n",
      "Collecting dpath<3,>=2.1.0 (from dvc)\n",
      "  Obtaining dependency information for dpath<3,>=2.1.0 from https://files.pythonhosted.org/packages/84/c8/10c2d41a0958b76e777c07a521d64c871ab9022520babb3e08fa7eeb0810/dpath-2.1.6-py3-none-any.whl.metadata\n",
      "  Downloading dpath-2.1.6-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting dvc-data<2.21.0,>=2.20.0 (from dvc)\n",
      "  Obtaining dependency information for dvc-data<2.21.0,>=2.20.0 from https://files.pythonhosted.org/packages/d1/f3/0b62e3c2711ed3647032176f44482e8c19ba7925e66fb53fdfd7f6ca4b1f/dvc_data-2.20.0-py3-none-any.whl.metadata\n",
      "  Downloading dvc_data-2.20.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting dvc-http>=2.29.0 (from dvc)\n",
      "  Downloading dvc_http-2.30.2-py3-none-any.whl (12 kB)\n",
      "Collecting dvc-render<1,>=0.3.1 (from dvc)\n",
      "  Obtaining dependency information for dvc-render<1,>=0.3.1 from https://files.pythonhosted.org/packages/e4/52/675239b9451c327a462fc66a09f4c1c96a5b5d90faf566864e35e15a7791/dvc_render-0.6.0-py3-none-any.whl.metadata\n",
      "  Downloading dvc_render-0.6.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting dvc-studio-client<1,>=0.13.0 (from dvc)\n",
      "  Obtaining dependency information for dvc-studio-client<1,>=0.13.0 from https://files.pythonhosted.org/packages/54/39/93df103f61f26fabcc4322226bb8aa187b945a309a14ca32e8fc4c3f41d1/dvc_studio_client-0.15.0-py3-none-any.whl.metadata\n",
      "  Downloading dvc_studio_client-0.15.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting dvc-task<1,>=0.3.0 (from dvc)\n",
      "  Obtaining dependency information for dvc-task<1,>=0.3.0 from https://files.pythonhosted.org/packages/93/8c/3aebe5c887c1277b671aa9d3931a836375ed2443ad37302c50d0ae84c50d/dvc_task-0.3.0-py3-none-any.whl.metadata\n",
      "  Downloading dvc_task-0.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting flatten-dict<1,>=0.4.1 (from dvc)\n",
      "  Downloading flatten_dict-0.4.2-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting flufl.lock<8,>=5 (from dvc)\n",
      "  Downloading flufl.lock-7.1.1-py3-none-any.whl (11 kB)\n",
      "Collecting funcy>=1.14 (from dvc)\n",
      "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
      "Collecting grandalf<1,>=0.7 (from dvc)\n",
      "  Downloading grandalf-0.8-py3-none-any.whl (41 kB)\n",
      "     ---------------------------------------- 0.0/41.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.8/41.8 kB ? eta 0:00:00\n",
      "Collecting gto<2,>=1.4.0 (from dvc)\n",
      "  Obtaining dependency information for gto<2,>=1.4.0 from https://files.pythonhosted.org/packages/8d/e4/0fd9dc427bcf2e009e3d2272d6704a0551597a30e4a5b984b2c2f9784cc0/gto-1.5.0-py3-none-any.whl.metadata\n",
      "  Downloading gto-1.5.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting hydra-core>=1.1 (from dvc)\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "     ---------------------------------------- 0.0/154.5 kB ? eta -:--:--\n",
      "     ----------------------------------- -- 143.4/154.5 kB 8.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 154.5/154.5 kB 4.5 MB/s eta 0:00:00\n",
      "Collecting iterative-telemetry>=0.0.7 (from dvc)\n",
      "  Downloading iterative_telemetry-0.0.8-py3-none-any.whl (10 kB)\n",
      "Collecting networkx>=2.5 (from dvc)\n",
      "  Obtaining dependency information for networkx>=2.5 from https://files.pythonhosted.org/packages/d5/f0/8fbc882ca80cf077f1b246c0e3c3465f7f415439bdea6b899f6b19f61f70/networkx-3.2.1-py3-none-any.whl.metadata\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: packaging>=19 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from dvc) (23.2)\n",
      "Collecting pathspec>=0.10.3 (from dvc)\n",
      "  Obtaining dependency information for pathspec>=0.10.3 from https://files.pythonhosted.org/packages/b4/2a/9b1be29146139ef459188f5e420a66e835dda921208db600b7037093891f/pathspec-0.11.2-py3-none-any.whl.metadata\n",
      "  Downloading pathspec-0.11.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting platformdirs<4,>=3.1.1 (from dvc)\n",
      "  Obtaining dependency information for platformdirs<4,>=3.1.1 from https://files.pythonhosted.org/packages/56/29/3ec311dc18804409ecf0d2b09caa976f3ae6215559306b5b530004e11156/platformdirs-3.11.0-py3-none-any.whl.metadata\n",
      "  Using cached platformdirs-3.11.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: psutil>=5.8 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from dvc) (5.9.6)\n",
      "Collecting pydot>=1.2.4 (from dvc)\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting pygtrie>=2.3.2 (from dvc)\n",
      "  Downloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: pyparsing>=2.4.7 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from dvc) (3.1.1)\n",
      "Requirement already satisfied: requests>=2.22 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from dvc) (2.31.0)\n",
      "Collecting rich>=12 (from dvc)\n",
      "  Obtaining dependency information for rich>=12 from https://files.pythonhosted.org/packages/be/2a/4e62ff633612f746f88618852a626bbe24226eba5e7ac90e91dcfd6a414e/rich-13.6.0-py3-none-any.whl.metadata\n",
      "  Downloading rich-13.6.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting ruamel.yaml>=0.17.11 (from dvc)\n",
      "  Obtaining dependency information for ruamel.yaml>=0.17.11 from https://files.pythonhosted.org/packages/57/db/4b74a29f5fd175aea3ff0d95f8230d9bb8a54e38d963c6f96065b9e2eef3/ruamel.yaml-0.18.5-py3-none-any.whl.metadata\n",
      "  Downloading ruamel.yaml-0.18.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting scmrepo<2,>=1.4.1 (from dvc)\n",
      "  Obtaining dependency information for scmrepo<2,>=1.4.1 from https://files.pythonhosted.org/packages/65/97/d8bb26cf1aaa09fcaae4d98d5e46ae2868114c8264e05070a61fefbe6645/scmrepo-1.4.1-py3-none-any.whl.metadata\n",
      "  Downloading scmrepo-1.4.1-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting shortuuid>=0.5 (from dvc)\n",
      "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
      "Collecting shtab<2,>=1.3.4 (from dvc)\n",
      "  Obtaining dependency information for shtab<2,>=1.3.4 from https://files.pythonhosted.org/packages/86/69/3a4873b36d65a1b8f4ee606f5a785b5babb9960385802de60d8455e2f8b6/shtab-1.6.4-py3-none-any.whl.metadata\n",
      "  Downloading shtab-1.6.4-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: tabulate>=0.8.7 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from dvc) (0.9.0)\n",
      "Collecting tomlkit>=0.11.1 (from dvc)\n",
      "  Obtaining dependency information for tomlkit>=0.11.1 from https://files.pythonhosted.org/packages/6e/43/159750d32481f16e34cc60090b53bc0a14314ad0c1f67a9bb64f3f3a0551/tomlkit-0.12.3-py3-none-any.whl.metadata\n",
      "  Downloading tomlkit-0.12.3-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting tqdm<5,>=4.63.1 (from dvc)\n",
      "  Obtaining dependency information for tqdm<5,>=4.63.1 from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting voluptuous>=0.11.7 (from dvc)\n",
      "  Obtaining dependency information for voluptuous>=0.11.7 from https://files.pythonhosted.org/packages/ce/03/7b91e5fa5b36947cb858c45bc50270fee86b3db6c2cd002eac2ee68c09a1/voluptuous-0.14.0-py3-none-any.whl.metadata\n",
      "  Downloading voluptuous-0.14.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting zc.lockfile>=1.2.1 (from dvc)\n",
      "  Downloading zc.lockfile-3.0.post1-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: six in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from configobj>=5.0.6->dvc) (1.16.0)\n",
      "Collecting dictdiffer>=0.8.1 (from dvc-data<2.21.0,>=2.20.0->dvc)\n",
      "  Downloading dictdiffer-0.9.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting dvc-objects<2,>=1.1.0 (from dvc-data<2.21.0,>=2.20.0->dvc)\n",
      "  Obtaining dependency information for dvc-objects<2,>=1.1.0 from https://files.pythonhosted.org/packages/cb/da/f849b088d313772e60be2db3268100d9e06b5c3530c10077529894cd5d5e/dvc_objects-1.2.0-py3-none-any.whl.metadata\n",
      "  Downloading dvc_objects-1.2.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting diskcache>=5.2.1 (from dvc-data<2.21.0,>=2.20.0->dvc)\n",
      "  Obtaining dependency information for diskcache>=5.2.1 from https://files.pythonhosted.org/packages/3f/27/4570e78fc0bf5ea0ca45eb1de3818a23787af9b390c0b0a0033a1b8236f9/diskcache-5.6.3-py3-none-any.whl.metadata\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting attrs>=21.3.0 (from dvc-data<2.21.0,>=2.20.0->dvc)\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "     ---------------------------------------- 0.0/61.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 61.2/61.2 kB ? eta 0:00:00\n",
      "Collecting sqltrie<1,>=0.8.0 (from dvc-data<2.21.0,>=2.20.0->dvc)\n",
      "  Obtaining dependency information for sqltrie<1,>=0.8.0 from https://files.pythonhosted.org/packages/ae/c8/2a590d30003d2c0a66571dc0c3fb2b143bffaf93fbc14f80c0489afb889e/sqltrie-0.8.0-py3-none-any.whl.metadata\n",
      "  Downloading sqltrie-0.8.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting fsspec[http] (from dvc-http>=2.29.0->dvc)\n",
      "  Obtaining dependency information for fsspec[http] from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n",
      "  Using cached fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp-retry>=2.5.0 (from dvc-http>=2.29.0->dvc)\n",
      "  Downloading aiohttp_retry-2.8.3-py3-none-any.whl (9.8 kB)\n",
      "Collecting dulwich (from dvc-studio-client<1,>=0.13.0->dvc)\n",
      "  Obtaining dependency information for dulwich from https://files.pythonhosted.org/packages/59/f6/1c2491bc270d7b66157c1f13203d2325ba0a210cfd62f600fab2ed16f6f3/dulwich-0.21.6-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading dulwich-0.21.6-cp311-cp311-win_amd64.whl.metadata (4.4 kB)\n",
      "Collecting celery<6,>=5.3.0 (from dvc-task<1,>=0.3.0->dvc)\n",
      "  Obtaining dependency information for celery<6,>=5.3.0 from https://files.pythonhosted.org/packages/a3/fb/0bcea0312649a374601d5a15092b2a3659801d578e70cec03aa72053ccaa/celery-5.3.5-py3-none-any.whl.metadata\n",
      "  Downloading celery-5.3.5-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting kombu<6,>=5.3.0 (from dvc-task<1,>=0.3.0->dvc)\n",
      "  Obtaining dependency information for kombu<6,>=5.3.0 from https://files.pythonhosted.org/packages/c4/05/0504dce43327e610cbe05c223691992bdb0212202bb830f105d57641665f/kombu-5.3.3-py3-none-any.whl.metadata\n",
      "  Downloading kombu-5.3.3-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: pywin32>=225 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from dvc-task<1,>=0.3.0->dvc) (306)\n",
      "Collecting atpublic>=2.3 (from flufl.lock<8,>=5->dvc)\n",
      "  Obtaining dependency information for atpublic>=2.3 from https://files.pythonhosted.org/packages/42/d5/f3c7110d3763af646150203b8bfe6932ab05a9b3e228c27d138babeb92ae/atpublic-4.0-py3-none-any.whl.metadata\n",
      "  Downloading atpublic-4.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting typer>=0.4.1 (from gto<2,>=1.4.0->dvc)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 0.0/45.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 45.9/45.9 kB ? eta 0:00:00\n",
      "Collecting pydantic!=2.0.0,<3,>=1.9.0 (from gto<2,>=1.4.0->dvc)\n",
      "  Obtaining dependency information for pydantic!=2.0.0,<3,>=1.9.0 from https://files.pythonhosted.org/packages/d7/10/ddfb9539a6e55f7dfd6c2b9b81d86fcba2761ba87eeb81f8b1012957dcdc/pydantic-2.5.0-py3-none-any.whl.metadata\n",
      "  Downloading pydantic-2.5.0-py3-none-any.whl.metadata (174 kB)\n",
      "     ---------------------------------------- 0.0/174.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 174.6/174.6 kB 5.3 MB/s eta 0:00:00\n",
      "Collecting semver>=3.0.0 (from gto<2,>=1.4.0->dvc)\n",
      "  Obtaining dependency information for semver>=3.0.0 from https://files.pythonhosted.org/packages/9a/77/0cc7a8a3bc7e53d07e8f47f147b92b0960e902b8254859f4aee5c4d7866b/semver-3.0.2-py3-none-any.whl.metadata\n",
      "  Downloading semver-3.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from gto<2,>=1.4.0->dvc) (0.4)\n",
      "Collecting omegaconf<2.4,>=2.2 (from hydra-core>=1.1->dvc)\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "     ---------------------------------------- 0.0/79.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 79.5/79.5 kB 4.6 MB/s eta 0:00:00\n",
      "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1->dvc)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "     ---------------------------------------- 0.0/117.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 117.0/117.0 kB 7.1 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting appdirs (from iterative-telemetry>=0.0.7->dvc)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting filelock (from iterative-telemetry>=0.0.7->dvc)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/81/54/84d42a0bee35edba99dee7b59a8d4970eccdd44b99fe728ed912106fc781/filelock-3.13.1-py3-none-any.whl.metadata\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from requests>=2.22->dvc) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from requests>=2.22->dvc) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from requests>=2.22->dvc) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from requests>=2.22->dvc) (2023.7.22)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=12->dvc)\n",
      "  Obtaining dependency information for markdown-it-py>=2.2.0 from https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl.metadata\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from rich>=12->dvc) (2.16.1)\n",
      "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.11->dvc)\n",
      "  Obtaining dependency information for ruamel.yaml.clib>=0.2.7 from https://files.pythonhosted.org/packages/ec/54/d8a795997921d87224c65d44499ca595a833093fb215b133f920c1062956/ruamel.yaml.clib-0.2.8-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading ruamel.yaml.clib-0.2.8-cp311-cp311-win_amd64.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: gitpython>3 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from scmrepo<2,>=1.4.1->dvc) (3.1.40)\n",
      "Collecting pygit2>=1.13.0 (from scmrepo<2,>=1.4.1->dvc)\n",
      "  Obtaining dependency information for pygit2>=1.13.0 from https://files.pythonhosted.org/packages/95/dc/86f76afa17ed49cef40373f9f2b7c3bb90615fde8983c0de1ed1a4acdc50/pygit2-1.13.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading pygit2-1.13.2-cp311-cp311-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting asyncssh<3,>=2.13.1 (from scmrepo<2,>=1.4.1->dvc)\n",
      "  Obtaining dependency information for asyncssh<3,>=2.13.1 from https://files.pythonhosted.org/packages/9f/42/72736d020c5bf2e2271d119e5907a41d871d2d0dacc118b2cb3d82b4fcf4/asyncssh-2.14.1-py3-none-any.whl.metadata\n",
      "  Downloading asyncssh-2.14.1-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from zc.lockfile>=1.2.1->dvc) (68.2.2)\n",
      "Collecting aiohttp (from aiohttp-retry>=2.5.0->dvc-http>=2.29.0->dvc)\n",
      "  Obtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/2e/9f/9c37b01fc6a37c92f139a4cd937a92f03ebbd75379cfd55e85ca1e571643/aiohttp-3.8.6-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading aiohttp-3.8.6-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting cryptography>=39.0 (from asyncssh<3,>=2.13.1->scmrepo<2,>=1.4.1->dvc)\n",
      "  Obtaining dependency information for cryptography>=39.0 from https://files.pythonhosted.org/packages/86/35/f03a42444866ef7f23134812a05012dcb509418214fb78ec848f28cd14b8/cryptography-41.0.5-cp37-abi3-win_amd64.whl.metadata\n",
      "  Using cached cryptography-41.0.5-cp37-abi3-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from asyncssh<3,>=2.13.1->scmrepo<2,>=1.4.1->dvc) (4.8.0)\n",
      "Collecting billiard<5.0,>=4.2.0 (from celery<6,>=5.3.0->dvc-task<1,>=0.3.0->dvc)\n",
      "  Obtaining dependency information for billiard<5.0,>=4.2.0 from https://files.pythonhosted.org/packages/50/8d/6e9fdeeab04d803abc5a715175f87e88893934d5590595eacff23ca12b07/billiard-4.2.0-py3-none-any.whl.metadata\n",
      "  Downloading billiard-4.2.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting click-didyoumean>=0.3.0 (from celery<6,>=5.3.0->dvc-task<1,>=0.3.0->dvc)\n",
      "  Downloading click_didyoumean-0.3.0-py3-none-any.whl (2.7 kB)\n",
      "Collecting click-plugins>=1.1.1 (from celery<6,>=5.3.0->dvc-task<1,>=0.3.0->dvc)\n",
      "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting click-repl>=0.2.0 (from celery<6,>=5.3.0->dvc-task<1,>=0.3.0->dvc)\n",
      "  Obtaining dependency information for click-repl>=0.2.0 from https://files.pythonhosted.org/packages/52/40/9d857001228658f0d59e97ebd4c346fe73e138c6de1bce61dc568a57c7f8/click_repl-0.3.0-py3-none-any.whl.metadata\n",
      "  Downloading click_repl-0.3.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: click<9.0,>=8.1.2 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from celery<6,>=5.3.0->dvc-task<1,>=0.3.0->dvc) (8.1.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from celery<6,>=5.3.0->dvc-task<1,>=0.3.0->dvc) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from celery<6,>=5.3.0->dvc-task<1,>=0.3.0->dvc) (2023.3)\n",
      "Collecting vine<6.0,>=5.1.0 (from celery<6,>=5.3.0->dvc-task<1,>=0.3.0->dvc)\n",
      "  Obtaining dependency information for vine<6.0,>=5.1.0 from https://files.pythonhosted.org/packages/03/ff/7c0c86c43b3cbb927e0ccc0255cb4057ceba4799cd44ae95174ce8e8b5b2/vine-5.1.0-py3-none-any.whl.metadata\n",
      "  Downloading vine-5.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from gitpython>3->scmrepo<2,>=1.4.1->dvc) (4.0.11)\n",
      "Collecting amqp<6.0.0,>=5.1.1 (from kombu<6,>=5.3.0->dvc-task<1,>=0.3.0->dvc)\n",
      "  Obtaining dependency information for amqp<6.0.0,>=5.1.1 from https://files.pythonhosted.org/packages/b3/f0/8e5be5d5e0653d9e1d02b1144efa33ff7d2963dfad07049e02c0fa9b2e8d/amqp-5.2.0-py3-none-any.whl.metadata\n",
      "  Downloading amqp-5.2.0-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=12->dvc)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from omegaconf<2.4,>=2.2->hydra-core>=1.1->dvc) (6.0.1)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=2.0.0,<3,>=1.9.0->gto<2,>=1.4.0->dvc)\n",
      "  Obtaining dependency information for annotated-types>=0.4.0 from https://files.pythonhosted.org/packages/28/78/d31230046e58c207284c6b2c4e8d96e6d3cb4e52354721b944d3e1ee4aa5/annotated_types-0.6.0-py3-none-any.whl.metadata\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.14.1 (from pydantic!=2.0.0,<3,>=1.9.0->gto<2,>=1.4.0->dvc)\n",
      "  Obtaining dependency information for pydantic-core==2.14.1 from https://files.pythonhosted.org/packages/15/8e/68d3a522ac31e782e3ae95ddc7b4ec4bea0f5c552a98f85abdee6696a607/pydantic_core-2.14.1-cp311-none-win_amd64.whl.metadata\n",
      "  Downloading pydantic_core-2.14.1-cp311-none-win_amd64.whl.metadata (6.6 kB)\n",
      "Collecting cffi>=1.16.0 (from pygit2>=1.13.0->scmrepo<2,>=1.4.1->dvc)\n",
      "  Obtaining dependency information for cffi>=1.16.0 from https://files.pythonhosted.org/packages/5a/c7/694814b3757878b29da39bc2f0cf9d20295f4c1e0a0bde7971708d5f23f8/cffi-1.16.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached cffi-1.16.0-cp311-cp311-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting orjson (from sqltrie<1,>=0.8.0->dvc-data<2.21.0,>=2.20.0->dvc)\n",
      "  Obtaining dependency information for orjson from https://files.pythonhosted.org/packages/5d/67/d7837cf0ac956e3c81c67dda3e8f2ffc60dd50ffc480ec7c17f2e22a36ae/orjson-3.9.10-cp311-none-win_amd64.whl.metadata\n",
      "  Downloading orjson-3.9.10-cp311-none-win_amd64.whl.metadata (50 kB)\n",
      "     ---------------------------------------- 0.0/50.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 50.5/50.5 kB 2.5 MB/s eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->aiohttp-retry>=2.5.0->dvc-http>=2.29.0->dvc)\n",
      "  Downloading multidict-6.0.4-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->aiohttp-retry>=2.5.0->dvc-http>=2.29.0->dvc)\n",
      "  Obtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->aiohttp-retry>=2.5.0->dvc-http>=2.29.0->dvc)\n",
      "  Downloading yarl-1.9.2-cp311-cp311-win_amd64.whl (60 kB)\n",
      "     ---------------------------------------- 0.0/60.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.2/60.2 kB ? eta 0:00:00\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->aiohttp-retry>=2.5.0->dvc-http>=2.29.0->dvc)\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/39/16/72d9ccd30815d0b37218348f053be37bc3d14288ac192a794a39990ac28e/frozenlist-1.4.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading frozenlist-1.4.0-cp311-cp311-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->aiohttp-retry>=2.5.0->dvc-http>=2.29.0->dvc)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting pycparser (from cffi>=1.16.0->pygit2>=1.13.0->scmrepo<2,>=1.4.1->dvc)\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.36 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from click-repl>=0.2.0->celery<6,>=5.3.0->dvc-task<1,>=0.3.0->dvc) (3.0.41)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython>3->scmrepo<2,>=1.4.1->dvc) (5.0.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\zacha\\documents\\cours9\\adde92applications of big data\\tp1\\envtp1\\lib\\site-packages (from prompt-toolkit>=3.0.36->click-repl>=0.2.0->celery<6,>=5.3.0->dvc-task<1,>=0.3.0->dvc) (0.2.10)\n",
      "Downloading dvc-3.29.0-py3-none-any.whl (429 kB)\n",
      "   ---------------------------------------- 0.0/429.9 kB ? eta -:--:--\n",
      "   -------------------------------- ------ 358.4/429.9 kB 11.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 429.9/429.9 kB 13.5 MB/s eta 0:00:00\n",
      "Downloading dpath-2.1.6-py3-none-any.whl (17 kB)\n",
      "Downloading dvc_data-2.20.0-py3-none-any.whl (68 kB)\n",
      "   ---------------------------------------- 0.0/68.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 68.0/68.0 kB ? eta 0:00:00\n",
      "Downloading dvc_render-0.6.0-py3-none-any.whl (19 kB)\n",
      "Downloading dvc_studio_client-0.15.0-py3-none-any.whl (13 kB)\n",
      "Downloading dvc_task-0.3.0-py3-none-any.whl (21 kB)\n",
      "Downloading gto-1.5.0-py3-none-any.whl (46 kB)\n",
      "   ---------------------------------------- 0.0/46.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 46.6/46.6 kB ? eta 0:00:00\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Downloading pathspec-0.11.2-py3-none-any.whl (29 kB)\n",
      "Using cached platformdirs-3.11.0-py3-none-any.whl (17 kB)\n",
      "Downloading rich-13.6.0-py3-none-any.whl (239 kB)\n",
      "   ---------------------------------------- 0.0/239.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 239.8/239.8 kB 15.3 MB/s eta 0:00:00\n",
      "Downloading ruamel.yaml-0.18.5-py3-none-any.whl (116 kB)\n",
      "   ---------------------------------------- 0.0/116.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 116.4/116.4 kB 6.6 MB/s eta 0:00:00\n",
      "Downloading scmrepo-1.4.1-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.0/58.0 kB ? eta 0:00:00\n",
      "Downloading shtab-1.6.4-py3-none-any.whl (13 kB)\n",
      "Downloading tomlkit-0.12.3-py3-none-any.whl (37 kB)\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Downloading voluptuous-0.14.0-py3-none-any.whl (30 kB)\n",
      "Downloading asyncssh-2.14.1-py3-none-any.whl (352 kB)\n",
      "   ---------------------------------------- 0.0/352.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 352.2/352.2 kB 11.0 MB/s eta 0:00:00\n",
      "Downloading atpublic-4.0-py3-none-any.whl (4.9 kB)\n",
      "Downloading celery-5.3.5-py3-none-any.whl (421 kB)\n",
      "   ---------------------------------------- 0.0/421.9 kB ? eta -:--:--\n",
      "   -------------------------- ------------ 286.7/421.9 kB 17.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 421.9/421.9 kB 8.8 MB/s eta 0:00:00\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.5/45.5 kB ? eta 0:00:00\n",
      "Downloading dulwich-0.21.6-cp311-cp311-win_amd64.whl (484 kB)\n",
      "   ---------------------------------------- 0.0/484.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 484.9/484.9 kB 31.6 MB/s eta 0:00:00\n",
      "Downloading dvc_objects-1.2.0-py3-none-any.whl (38 kB)\n",
      "Using cached fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "Downloading kombu-5.3.3-py3-none-any.whl (199 kB)\n",
      "   ---------------------------------------- 0.0/199.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 199.1/199.1 kB 12.6 MB/s eta 0:00:00\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "   ---------------------------------------- 0.0/87.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 87.5/87.5 kB ? eta 0:00:00\n",
      "Downloading pydantic-2.5.0-py3-none-any.whl (407 kB)\n",
      "   ---------------------------------------- 0.0/407.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 407.5/407.5 kB 24.8 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.14.1-cp311-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 0.9/1.9 MB 27.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 19.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 20.0 MB/s eta 0:00:00\n",
      "Downloading pygit2-1.13.2-cp311-cp311-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 1.1/1.2 MB 33.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 19.2 MB/s eta 0:00:00\n",
      "Downloading ruamel.yaml.clib-0.2.8-cp311-cp311-win_amd64.whl (118 kB)\n",
      "   ---------------------------------------- 0.0/118.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 118.0/118.0 kB ? eta 0:00:00\n",
      "Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
      "Downloading sqltrie-0.8.0-py3-none-any.whl (17 kB)\n",
      "Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading aiohttp-3.8.6-cp311-cp311-win_amd64.whl (322 kB)\n",
      "   ---------------------------------------- 0.0/322.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 322.6/322.6 kB ? eta 0:00:00\n",
      "Downloading amqp-5.2.0-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.9/50.9 kB ? eta 0:00:00\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading billiard-4.2.0-py3-none-any.whl (86 kB)\n",
      "   ---------------------------------------- 0.0/86.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 86.7/86.7 kB ? eta 0:00:00\n",
      "Using cached cffi-1.16.0-cp311-cp311-win_amd64.whl (181 kB)\n",
      "Downloading click_repl-0.3.0-py3-none-any.whl (10 kB)\n",
      "Using cached cryptography-41.0.5-cp37-abi3-win_amd64.whl (2.7 MB)\n",
      "Downloading vine-5.1.0-py3-none-any.whl (9.6 kB)\n",
      "Downloading orjson-3.9.10-cp311-none-win_amd64.whl (135 kB)\n",
      "   ---------------------------------------- 0.0/135.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 135.0/135.0 kB 7.8 MB/s eta 0:00:00\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 44.9/44.9 kB ? eta 0:00:00\n",
      "Building wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (pyproject.toml): started\n",
      "  Building wheel for antlr4-python3-runtime (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144579 sha256=e4cbdd1cd74b0e2b742fe27f77532a72ef62646541692faa43fd557c280d9393\n",
      "  Stored in directory: c:\\users\\zacha\\appdata\\local\\pip\\cache\\wheels\\1a\\97\\32\\461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: voluptuous, pygtrie, funcy, dictdiffer, appdirs, antlr4-python3-runtime, zc.lockfile, vine, tqdm, tomlkit, shtab, shortuuid, semver, ruamel.yaml.clib, pydot, pydantic-core, pycparser, platformdirs, pathspec, orjson, omegaconf, networkx, multidict, mdurl, grandalf, fsspec, frozenlist, flatten-dict, filelock, dvc-render, dulwich, dpath, distro, diskcache, configobj, billiard, attrs, atpublic, async-timeout, annotated-types, yarl, typer, sqltrie, ruamel.yaml, pydantic, markdown-it-py, iterative-telemetry, hydra-core, flufl.lock, dvc-studio-client, dvc-objects, click-repl, click-plugins, click-didyoumean, cffi, amqp, aiosignal, rich, pygit2, kombu, dvc-data, cryptography, aiohttp, celery, asyncssh, aiohttp-retry, scmrepo, dvc-task, dvc-http, gto, dvc\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 4.0.0\n",
      "    Uninstalling platformdirs-4.0.0:\n",
      "      Successfully uninstalled platformdirs-4.0.0\n",
      "Successfully installed aiohttp-3.8.6 aiohttp-retry-2.8.3 aiosignal-1.3.1 amqp-5.2.0 annotated-types-0.6.0 antlr4-python3-runtime-4.9.3 appdirs-1.4.4 async-timeout-4.0.3 asyncssh-2.14.1 atpublic-4.0 attrs-23.1.0 billiard-4.2.0 celery-5.3.5 cffi-1.16.0 click-didyoumean-0.3.0 click-plugins-1.1.1 click-repl-0.3.0 configobj-5.0.8 cryptography-41.0.5 dictdiffer-0.9.0 diskcache-5.6.3 distro-1.8.0 dpath-2.1.6 dulwich-0.21.6 dvc-3.29.0 dvc-data-2.20.0 dvc-http-2.30.2 dvc-objects-1.2.0 dvc-render-0.6.0 dvc-studio-client-0.15.0 dvc-task-0.3.0 filelock-3.13.1 flatten-dict-0.4.2 flufl.lock-7.1.1 frozenlist-1.4.0 fsspec-2023.10.0 funcy-2.0 grandalf-0.8 gto-1.5.0 hydra-core-1.3.2 iterative-telemetry-0.0.8 kombu-5.3.3 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.0.4 networkx-3.2.1 omegaconf-2.3.0 orjson-3.9.10 pathspec-0.11.2 platformdirs-3.11.0 pycparser-2.21 pydantic-2.5.0 pydantic-core-2.14.1 pydot-1.4.2 pygit2-1.13.2 pygtrie-2.5.0 rich-13.6.0 ruamel.yaml-0.18.5 ruamel.yaml.clib-0.2.8 scmrepo-1.4.1 semver-3.0.2 shortuuid-1.0.11 shtab-1.6.4 sqltrie-0.8.0 tomlkit-0.12.3 tqdm-4.66.1 typer-0.9.0 vine-5.1.0 voluptuous-0.14.0 yarl-1.9.2 zc.lockfile-3.0.post1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install dvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisez un projet dvc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized DVC repository.\n",
      "\n",
      "You can now commit the changes to git.\n",
      "\n",
      "+---------------------------------------------------------------------+\n",
      "|                                                                     |\n",
      "|        DVC has enabled anonymous aggregate usage analytics.         |\n",
      "|     Read the analytics documentation (and how to opt-out) here:     |\n",
      "|             <https://dvc.org/doc/user-guide/analytics>              |\n",
      "|                                                                     |\n",
      "+---------------------------------------------------------------------+\n",
      "\n",
      "What's next?\n",
      "------------\n",
      "- Check out the documentation: <https://dvc.org/doc>\n",
      "- Get help and share ideas: <https://dvc.org/chat>\n",
      "- Star us on GitHub: <https://github.com/iterative/dvc>\n"
     ]
    }
   ],
   "source": [
    "!dvc init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardez ce qui a été créé avec un git status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\n",
      "Your branch is up to date with 'origin/master'.\n",
      "\n",
      "Changes to be committed:\n",
      "  (use \"git restore --staged <file>...\" to unstage)\n",
      "\tnew file:   .dvc/.gitignore\n",
      "\tnew file:   .dvc/config\n",
      "\tnew file:   .dvcignore\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajoutez les nouveaux fichiers dans le fichier .dvc : le .gitignore, le fichier config, et le .dvcignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 1d98e7b] initialize repo\n",
      " 3 files changed, 6 insertions(+)\n",
      " create mode 100644 .dvc/.gitignore\n",
      " create mode 100644 .dvc/config\n",
      " create mode 100644 .dvcignore\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"initialize repo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons ajouter un stockage distant dans le cloud. Ici nous allons utiliser un stockage dans un dossier local pour les besoins du tp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting 'dvc-remote' as a default remote.\n"
     ]
    }
   ],
   "source": [
    "!dvc remote add -d dvc-remote /tmp/dvc-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut regarder le contenu du fichier dvc config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[core]\n",
      "    remote = dvc-remote\n",
      "['remote \"dvc-remote\"']\n",
      "    url = /tmp/dvc-storage\n"
     ]
    }
   ],
   "source": [
    "!type .dvc\\config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous constatons que l'url locale a bien été ajouté. Nous pouvons commiter ces changements à git."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 819acdb] add remote storage\n",
      " 1 file changed, 4 insertions(+)\n"
     ]
    }
   ],
   "source": [
    "!git commit .dvc/config -m \"add remote storage\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons créer un dossier data pour stocker notre dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Télécharger notre dataset et le placer dans le dossier data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataset_source_url = \"https://archive.ics.uci.edu/static/public/275/bike+sharing+dataset.zip\"\n",
    "\n",
    "content = requests.get(dataset_source_url).content\n",
    "with zipfile.ZipFile(io.BytesIO(content)) as arc:\n",
    "    raw_data = pd.read_csv(arc.open(\"hour.csv\"), header=0, sep=',', parse_dates=['dteday'], index_col='dteday')\n",
    "\n",
    "raw_data.to_csv(path_or_buf=\"data/hour.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichons le contenu de data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C s'appelle OS\n",
      " Le num�ro de s�rie du volume est 506E-5B85\n",
      "\n",
      " R�pertoire de c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\data\n",
      "\n",
      "15/11/2023  11:06    <DIR>          .\n",
      "15/11/2023  11:05    <DIR>          ..\n",
      "15/11/2023  11:06         1�161�688 hour.csv\n",
      "               1 fichier(s)        1�161�688 octets\n",
      "               2 R�p(s)  340�687�327�232 octets libres\n"
     ]
    }
   ],
   "source": [
    "!dir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nous voulons commencer à suivre les changements d'un fichier il nous suffit de l'ajouter via dvc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add 'data\\.gitignore' 'data\\hour.csv.dvc'\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⠋ Checking graph\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!dvc add data/hour.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons regarder de nouveau ce qu'il y a dans notre dossier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C s'appelle OS\n",
      " Le num�ro de s�rie du volume est 506E-5B85\n",
      "\n",
      " R�pertoire de c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\data\n",
      "\n",
      "15/11/2023  11:06    <DIR>          .\n",
      "15/11/2023  11:05    <DIR>          ..\n",
      "15/11/2023  11:06                11 .gitignore\n",
      "15/11/2023  11:06         1�161�688 hour.csv\n",
      "15/11/2023  11:06                96 hour.csv.dvc\n",
      "               3 fichier(s)        1�161�795 octets\n",
      "               2 R�p(s)  340�685�553�664 octets libres\n"
     ]
    }
   ],
   "source": [
    "!dir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voyons un nouveau fichier .dvc.   \n",
    "Si nous regardons à l'intérieur : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outs:\n",
      "- md5: 0cde6c4b0e77e95a1ff808ddc9da446a\n",
      "  size: 1161688\n",
      "  hash: md5\n",
      "  path: hour.csv\n"
     ]
    }
   ],
   "source": [
    "!type data\\hour.csv.dvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que le fichier contient des informations à propos de notre csv :  \n",
    "- Un hash du fichier \n",
    "- l'algorithme de hashage utilisé\n",
    "- la taille\n",
    "- le chemin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un gitignore a été créé par défaut, si on regarde à l'intérieur : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hour.csv\n"
     ]
    }
   ],
   "source": [
    "!type data\\.gitignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir que notre csv y est renseigné."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant ajoutons le nouveau fichier data/hour.csv.dvc et le fichier data/.gitignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add data/.gitignore data/hour.csv.dvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "et commitons le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master aed3bce] add .dvc file to track hours.csv file\n",
      " 2 files changed, 6 insertions(+)\n",
      " create mode 100644 data/.gitignore\n",
      " create mode 100644 data/hour.csv.dvc\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"add .dvc file to track hours.csv file\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une bonne idée est de créer un tag pour chaque version de notre dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git tag -a v1 -m \"raw data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre donnée est toujours sur notre dossier en local, maintenant nous devons l'envoyer sur notre stokage distant (qui pour rappel et en fait un autre dossier local). Pour ça nous utilisons la commande dvc push."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 file pushed\n"
     ]
    }
   ],
   "source": [
    "!dvc push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut regarder dans notre \"remote storage\" ce que nous avons :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C s'appelle OS\n",
      " Le num�ro de s�rie du volume est 506E-5B85\n",
      "\n",
      " R�pertoire de c:\\tmp\\dvc-storage\n",
      "\n",
      "15/11/2023  11:08    <DIR>          .\n",
      "15/11/2023  11:08    <DIR>          ..\n",
      "15/11/2023  11:08    <DIR>          files\n",
      "               0 fichier(s)                0 octets\n",
      "\n",
      " R�pertoire de c:\\tmp\\dvc-storage\\files\n",
      "\n",
      "15/11/2023  11:08    <DIR>          .\n",
      "15/11/2023  11:08    <DIR>          ..\n",
      "15/11/2023  11:08    <DIR>          md5\n",
      "               0 fichier(s)                0 octets\n",
      "\n",
      " R�pertoire de c:\\tmp\\dvc-storage\\files\\md5\n",
      "\n",
      "15/11/2023  11:08    <DIR>          .\n",
      "15/11/2023  11:08    <DIR>          ..\n",
      "15/11/2023  11:08    <DIR>          0c\n",
      "               0 fichier(s)                0 octets\n",
      "\n",
      " R�pertoire de c:\\tmp\\dvc-storage\\files\\md5\\0c\n",
      "\n",
      "15/11/2023  11:08    <DIR>          .\n",
      "15/11/2023  11:08    <DIR>          ..\n",
      "15/11/2023  11:08         1�161�688 de6c4b0e77e95a1ff808ddc9da446a\n",
      "               1 fichier(s)        1�161�688 octets\n",
      "\n",
      "     Total des fichiers list�s�:\n",
      "               1 fichier(s)        1�161�688 octets\n",
      "              11 R�p(s)  340�679�917�568 octets libres\n"
     ]
    }
   ],
   "source": [
    "!dir /s \\tmp\\dvc-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons voir que notre fichier est présent dans le dossier mais avec un nom différent, ce nom correspond au hash de la donnée du fichier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nos données sont sauvegardées à distance, nous pouvons les supprimer localement. Sauf le fichier .dvc ! Car sinon vous perdrez le lien avec vos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C s'appelle OS\n",
      " Le num�ro de s�rie du volume est 506E-5B85\n",
      "\n",
      " R�pertoire de c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\data\n",
      "\n",
      "15/11/2023  11:06    <DIR>          .\n",
      "15/11/2023  11:05    <DIR>          ..\n",
      "15/11/2023  11:06                11 .gitignore\n",
      "15/11/2023  11:06         1�161�688 hour.csv\n",
      "15/11/2023  11:06                96 hour.csv.dvc\n",
      "               3 fichier(s)        1�161�795 octets\n",
      "               2 R�p(s)  340�681�932�800 octets libres\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "!dir data\n",
    "!del data\\hour.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C s'appelle OS\n",
      " Le num�ro de s�rie du volume est 506E-5B85\n",
      "\n",
      " R�pertoire de c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\data\n",
      "\n",
      "15/11/2023  11:06    <DIR>          .\n",
      "15/11/2023  11:05    <DIR>          ..\n",
      "15/11/2023  11:06                11 .gitignore\n",
      "15/11/2023  11:06         1�161�688 hour.csv\n",
      "15/11/2023  11:06                96 hour.csv.dvc\n",
      "               3 fichier(s)        1�161�795 octets\n",
      "               2 R�p(s)  340�681�928�704 octets libres\n"
     ]
    }
   ],
   "source": [
    "!dir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un autre emplacement où vos données résident est le dossier .dvc/cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Le fichier sp�cifi� est introuvable.\n",
      "Le chemin d'acc�s sp�cifi� est introuvable.\n"
     ]
    }
   ],
   "source": [
    "!dir .dvc\\cache\\files\\md5\\0a\\1c63297d478edfdcc18433bb509cd5\n",
    "!type .dvc\\cache\\files\\md5\\0a\\1c63297d478edfdcc18433bb509cd5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous supprimons aussi les données à l'intérieur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rd /s /q .dvc\\cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nous voulons récupérer nos données localement, nous pouvons utiliser dvc pull pour récupérer les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 file fetched\n"
     ]
    }
   ],
   "source": [
    "!dvc pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nous regardons dans le dossier data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C s'appelle OS\n",
      " Le num�ro de s�rie du volume est 506E-5B85\n",
      "\n",
      " R�pertoire de c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\data\n",
      "\n",
      "15/11/2023  11:06    <DIR>          .\n",
      "15/11/2023  11:05    <DIR>          ..\n",
      "15/11/2023  11:06                11 .gitignore\n",
      "15/11/2023  11:06         1�161�688 hour.csv\n",
      "15/11/2023  11:06                96 hour.csv.dvc\n",
      "               3 fichier(s)        1�161�795 octets\n",
      "               2 R�p(s)  340�682�764�288 octets libres\n"
     ]
    }
   ],
   "source": [
    "!dir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre fichier est de retour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant modifions nos données !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C s'appelle OS\n",
      " Le num�ro de s�rie du volume est 506E-5B85\n",
      "\n",
      " R�pertoire de c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\data\n",
      "\n",
      "15/11/2023  11:06         1�161�688 hour.csv\n",
      "               1 fichier(s)        1�161�688 octets\n",
      "               0 R�p(s)  340�682�764�288 octets libres\n"
     ]
    }
   ],
   "source": [
    "!dir data\\hour.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sed -i '2,1001d' data/hour.csv\n",
    "!powershell -Command \"(Get-Content data\\hour.csv | Select-Object -Skip 1000) | Set-Content data\\hour.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C s'appelle OS\n",
      " Le num�ro de s�rie du volume est 506E-5B85\n",
      "\n",
      " R�pertoire de c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\data\n",
      "\n",
      "15/11/2023  11:11         1�098�095 hour.csv\n",
      "               1 fichier(s)        1�098�095 octets\n",
      "               0 R�p(s)  340�682�588�160 octets libres\n"
     ]
    }
   ],
   "source": [
    "!dir data\\hour.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et répétons les opérations précédantes : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⠋ Checking graph\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add 'data\\hour.csv.dvc'\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n"
     ]
    }
   ],
   "source": [
    "!dvc add data/hour.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add data/hour.csv.dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: pathspec 'remove' did not match any file(s) known to git\n",
      "error: pathspec '1000' did not match any file(s) known to git\n",
      "error: pathspec 'lines'' did not match any file(s) known to git\n"
     ]
    }
   ],
   "source": [
    "!git commit -m 'data: remove 1000 lines'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git tag -a v2 -m \"removed 1000 lines\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 file pushed\n"
     ]
    }
   ],
   "source": [
    "!dvc push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Le chemin d'acc�s sp�cifi� est introuvable.\n"
     ]
    }
   ],
   "source": [
    "!rd /s /q .data\\hour.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rd /s /q .dvc\\cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant regarder dans notre git log, et voir l'historique des modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commit aed3bcef3ae76d32855bf41ebd6bb50dbe8420ce\n",
      "Author: zacharyak30 <zachary.akakpo@efrei.net>\n",
      "Date:   Wed Nov 15 11:07:43 2023 +0100\n",
      "\n",
      "    add .dvc file to track hours.csv file\n",
      "\n",
      "commit 819acdbfcbbb1cdae256de97aa9efe0a064cc872\n",
      "Author: zacharyak30 <zachary.akakpo@efrei.net>\n",
      "Date:   Wed Nov 15 11:05:50 2023 +0100\n",
      "\n",
      "    add remote storage\n",
      "\n",
      "commit 1d98e7badf3707887ee1e7fc6ce6d96ebe6a1a2e\n",
      "Author: zacharyak30 <zachary.akakpo@efrei.net>\n",
      "Date:   Wed Nov 15 11:04:50 2023 +0100\n",
      "\n",
      "    initialize repo\n",
      "\n",
      "commit ce791de11881aa46b8441aea859084585b02b952\n",
      "Author: zacharyak30 <zachary.akakpo@efrei.net>\n",
      "Date:   Wed Nov 15 11:01:48 2023 +0100\n",
      "\n",
      "    starting\n"
     ]
    }
   ],
   "source": [
    "!git log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour accéder et extraire des versions spécifiques de nos données nous pouvons utiliser le package dvc en python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dvc==3.29.0\n",
      "dvc-data==2.20.0\n",
      "dvc-http==2.30.2\n",
      "dvc-objects==1.2.0\n",
      "dvc-render==0.6.0\n",
      "dvc-studio-client==0.15.0\n",
      "dvc-task==0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | findstr dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dvc\n",
    "import dvc.api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\zacha\\\\Documents\\\\CourS9\\\\ADDE92Applications of Big Data\\\\TP1'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"./data/hour.csv\" \n",
    "repo = 'c:\\\\Users\\\\zacha\\\\Documents\\\\CourS9\\\\ADDE92Applications of Big Data\\\\TP1'  \n",
    "version = \"v1\"  \n",
    "\n",
    "data_url = dvc.api.get_url(\n",
    "    path=path, \n",
    "    repo=repo,\n",
    "    rev=version\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:5000\n"
     ]
    }
   ],
   "source": [
    "MLFLOW_TRACKING_URI = \"http://localhost:5000\"\n",
    "print(MLFLOW_TRACKING_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment set to ML_EXP_WITH_DVC\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "config_exp(name=\"ML_EXP_WITH_DVC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       dteday  instant  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
      "0  2011-01-01        1       1   0     1   0        0        6           0   \n",
      "1  2011-01-01        2       1   0     1   1        0        6           0   \n",
      "2  2011-01-01        3       1   0     1   2        0        6           0   \n",
      "3  2011-01-01        4       1   0     1   3        0        6           0   \n",
      "4  2011-01-01        5       1   0     1   4        0        6           0   \n",
      "\n",
      "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
      "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
      "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
      "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
      "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
      "4           1  0.24  0.2879  0.75        0.0       0           1    1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(data_url, sep=\",\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, time\n",
    "\n",
    "data.index = raw_data.apply(\n",
    "    lambda row: datetime.combine(row.name, time(hour=int(row['hr']))), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dteday</th>\n",
       "      <th>instant</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-01 00:00:00</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01 01:00:00</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01 02:00:00</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01 03:00:00</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01 04:00:00</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         dteday  instant  season  yr  mnth  hr  holiday  \\\n",
       "2011-01-01 00:00:00  2011-01-01        1       1   0     1   0        0   \n",
       "2011-01-01 01:00:00  2011-01-01        2       1   0     1   1        0   \n",
       "2011-01-01 02:00:00  2011-01-01        3       1   0     1   2        0   \n",
       "2011-01-01 03:00:00  2011-01-01        4       1   0     1   3        0   \n",
       "2011-01-01 04:00:00  2011-01-01        5       1   0     1   4        0   \n",
       "\n",
       "                     weekday  workingday  weathersit  temp   atemp   hum  \\\n",
       "2011-01-01 00:00:00        6           0           1  0.24  0.2879  0.81   \n",
       "2011-01-01 01:00:00        6           0           1  0.22  0.2727  0.80   \n",
       "2011-01-01 02:00:00        6           0           1  0.22  0.2727  0.80   \n",
       "2011-01-01 03:00:00        6           0           1  0.24  0.2879  0.75   \n",
       "2011-01-01 04:00:00        6           0           1  0.24  0.2879  0.75   \n",
       "\n",
       "                     windspeed  casual  registered  cnt  \n",
       "2011-01-01 00:00:00        0.0       3          13   16  \n",
       "2011-01-01 01:00:00        0.0       8          32   40  \n",
       "2011-01-01 02:00:00        0.0       5          27   32  \n",
       "2011-01-01 03:00:00        0.0       3          10   13  \n",
       "2011-01-01 04:00:00        0.0       0           1    1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant utiliser 2 mécanismes pour ajouter plus d'informations sur notre jeu de données dans MLFlow :    \n",
    "-Grâce à dvc, nous avons maintenant des liens vers les différentes versions de notre jeu de données.   \n",
    "-Nous pouvons l'utiliser en combinaison avec le module mlflow.data pour ajouter plus d'informations sur notre jeu de données.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'cnt'\n",
    "prediction = 'prediction'\n",
    "numerical_features = ['temp', 'atemp', 'hum', 'windspeed', 'hr', 'weekday']\n",
    "categorical_features = ['season', 'holiday', 'workingday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2011-01-01 00:00:00'\n",
    "end_date = '2011-01-28 23:00:00'\n",
    "dataset = data.loc[start_date:end_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         dteday  instant  season  yr  mnth  hr  holiday  \\\n",
      "2011-01-01 00:00:00  2011-01-01        1       1   0     1   0        0   \n",
      "2011-01-01 01:00:00  2011-01-01        2       1   0     1   1        0   \n",
      "2011-01-01 02:00:00  2011-01-01        3       1   0     1   2        0   \n",
      "2011-01-01 03:00:00  2011-01-01        4       1   0     1   3        0   \n",
      "2011-01-01 04:00:00  2011-01-01        5       1   0     1   4        0   \n",
      "...                         ...      ...     ...  ..   ...  ..      ...   \n",
      "2011-01-28 19:00:00  2011-01-28      614       1   0     1  19        0   \n",
      "2011-01-28 20:00:00  2011-01-28      615       1   0     1  20        0   \n",
      "2011-01-28 21:00:00  2011-01-28      616       1   0     1  21        0   \n",
      "2011-01-28 22:00:00  2011-01-28      617       1   0     1  22        0   \n",
      "2011-01-28 23:00:00  2011-01-28      618       1   0     1  23        0   \n",
      "\n",
      "                     weekday  workingday  weathersit  temp   atemp   hum  \\\n",
      "2011-01-01 00:00:00        6           0           1  0.24  0.2879  0.81   \n",
      "2011-01-01 01:00:00        6           0           1  0.22  0.2727  0.80   \n",
      "2011-01-01 02:00:00        6           0           1  0.22  0.2727  0.80   \n",
      "2011-01-01 03:00:00        6           0           1  0.24  0.2879  0.75   \n",
      "2011-01-01 04:00:00        6           0           1  0.24  0.2879  0.75   \n",
      "...                      ...         ...         ...   ...     ...   ...   \n",
      "2011-01-28 19:00:00        5           1           2  0.24  0.2424  0.75   \n",
      "2011-01-28 20:00:00        5           1           2  0.24  0.2273  0.70   \n",
      "2011-01-28 21:00:00        5           1           2  0.22  0.2273  0.75   \n",
      "2011-01-28 22:00:00        5           1           1  0.24  0.2121  0.65   \n",
      "2011-01-28 23:00:00        5           1           1  0.24  0.2273  0.60   \n",
      "\n",
      "                     windspeed  casual  registered  cnt  \n",
      "2011-01-01 00:00:00     0.0000       3          13   16  \n",
      "2011-01-01 01:00:00     0.0000       8          32   40  \n",
      "2011-01-01 02:00:00     0.0000       5          27   32  \n",
      "2011-01-01 03:00:00     0.0000       3          10   13  \n",
      "2011-01-01 04:00:00     0.0000       0           1    1  \n",
      "...                        ...     ...         ...  ...  \n",
      "2011-01-28 19:00:00     0.1343       5          84   89  \n",
      "2011-01-28 20:00:00     0.1940       1          61   62  \n",
      "2011-01-28 21:00:00     0.1343       1          57   58  \n",
      "2011-01-28 22:00:00     0.3582       0          26   26  \n",
      "2011-01-28 23:00:00     0.2239       1          22   23  \n",
      "\n",
      "[618 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la même cellule : \n",
    "\n",
    "1)\n",
    "- Aller voir la documentation du module mlflow data et importer le bon objet pour les données pandas.  \n",
    "- Créer un run avec la date, l'heure, etc. comme nom.    \n",
    "- pour l'entrainement vous pouvez utiliser le chemin vers votre dataset lors de la création de votre dataframe.   \n",
    "\n",
    "2) \n",
    "- logger le dataset avec la méthode appropriée.  \n",
    "- logger également le chemin vers votre dataset.   \n",
    "- logger la version du dataset utilisée.   \n",
    "\n",
    "3)\n",
    "- Créer un fichier texte et logger le en tant qu'artifact. Dans ce fichier vous pourrez indiquer la colonne qui a servi de target, les features numériques et les features catégorielles.\n",
    "\n",
    "4)\n",
    "- N'oubliez pas d'utiliser la fonction mlflow.end_run() si vous n'avez pas utilisez de with pour le run.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import mlflow.sklearn\n",
    "# 1)\n",
    "from mlflow.tracking import MlflowClient\n",
    "client = MlflowClient()\n",
    "\n",
    "# Create a run with date, time as name\n",
    "run_name = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "run = client.create_run(\"0\", run_name=run_name)\n",
    "\n",
    "# Load your dataset\n",
    "data_path = path\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# 2)\n",
    "# Log the dataset\n",
    "mlflow.sklearn.log_model(clf,\"my_model\") \n",
    "mlflow.log_param(\"data_path\", data_path)\n",
    "mlflow.log_param(\"data_version\", \"v1\")  # replace with your data version\n",
    "\n",
    "# 3)\n",
    "# Create a text file and log it as an artifact\n",
    "with open(\"info.txt\", \"w\") as f:\n",
    "    f.write(\"Target column: target\\n\")  # replace with your target column\n",
    "    f.write(\"Numerical features: feature1, feature2\\n\")  # replace with your numerical features\n",
    "    f.write(\"Categorical features: feature3, feature4\\n\")  # replace with your categorical features\n",
    "\n",
    "mlflow.log_artifact(\"info.txt\")\n",
    "\n",
    "# 4)\n",
    "# End the run\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Déploiement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.Trouver comment transitionner un modèle en état staging et ensuite dans l'état production. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\zacha\\Documents\\CourS9\\ADDE92Applications of Big Data\\TP1\\envTP1\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Successfully registered model 'sk-learn-random-forest-reg-model'.\n",
      "2023/11/15 13:34:56 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: sk-learn-random-forest-reg-model, version 1\n",
      "Created version '1' of model 'sk-learn-random-forest-reg-model'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    X, y = make_regression(n_features=4, n_informative=2, random_state=0, shuffle=False)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    params = {\"max_depth\": 2, \"random_state\": 42}\n",
    "    model = RandomForestRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Infer the model signature\n",
    "    y_pred = model.predict(X_test)\n",
    "    signature = infer_signature(X_test, y_pred)\n",
    "\n",
    "    # Log parameters and metrics using the MLflow APIs\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metrics({\"mse\": mean_squared_error(y_test, y_pred)})\n",
    "\n",
    "    # Log the sklearn model and register as version 1\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=model,\n",
    "        artifact_path=\"sklearn-model\",\n",
    "        signature=signature,\n",
    "        registered_model_name=\"sk-learn-random-forest-reg-model\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered model 'sk-learn-random-forest-reg-model' version 1\n"
     ]
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "model_name = \"sk-learn-random-forest-reg-model\"\n",
    "model_version_details = client.get_latest_versions(model_name, stages=[\"None\"])\n",
    "model_version = model_version_details[0].version\n",
    "\n",
    "print(f\"Registered model '{model_name}' version {model_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ModelVersion: aliases=[], creation_timestamp=1700051696983, current_stage='Production', description='', last_updated_timestamp=1700051856563, name='sk-learn-random-forest-reg-model', run_id='98e6a895b354471b992e9e9565617dad', run_link='', source='mlflow-artifacts:/188222493590758486/98e6a895b354471b992e9e9565617dad/artifacts/sklearn-model', status='READY', status_message='', tags={}, user_id='', version='1'>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transition model version 1 of model 'my_model' to 'Staging'\n",
    "client.transition_model_version_stage(\n",
    "  name=model_name,\n",
    "  version=model_version,\n",
    "  stage=\"Staging\",\n",
    ")\n",
    "\n",
    "# Later, transition the same model version to 'Production'\n",
    "client.transition_model_version_stage(\n",
    "  name=model_name,\n",
    "  version=model_version,\n",
    "  stage=\"Production\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.Créer une fonction qui récupère la version du modèle avec les meilleurs metrics d'entraînement et qui transitionne ce modèle dans l'état production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promoted version 1 of model sk-learn-random-forest-reg-model to 'Production'\n"
     ]
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "def promote_best_model_to_production(model_name, metric_name):\n",
    "    client = MlflowClient()\n",
    "\n",
    "    # Get a list of all versions of the model\n",
    "    versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "\n",
    "    # Find the version with the best metric\n",
    "    best_version = None\n",
    "    best_metric = float('inf')\n",
    "    for version in versions:\n",
    "        run_id = version.run_id\n",
    "        run = client.get_run(run_id)\n",
    "        if metric_name in run.data.metrics:\n",
    "            metric_value = run.data.metrics[metric_name]\n",
    "            if metric_value < best_metric:\n",
    "                best_metric = metric_value\n",
    "                best_version = version.version\n",
    "\n",
    "    if best_version is not None:\n",
    "        # Transition the best version to 'Production'\n",
    "        client.transition_model_version_stage(\n",
    "            name=model_name,\n",
    "            version=best_version,\n",
    "            stage=\"Production\",\n",
    "        )\n",
    "        print(f\"Promoted version {best_version} of model {model_name} to 'Production'\")\n",
    "    else:\n",
    "        print(f\"No suitable version found for model {model_name}\")\n",
    "\n",
    "# Usage:\n",
    "promote_best_model_to_production(model_name, \"mse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Le serveur mlflow peut vous fournir des prédictions à partir des modèles enregistrés. Faites en sorte d'obtenir une prédiction de votre dernier modèle en requêtant le serveur mlflow. (Voir : https://mlflow.org/docs/latest/models.html#command-line-interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 5/5 [00:02<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.36355607] 49.822907447421905\n"
     ]
    }
   ],
   "source": [
    "X, y = make_regression(n_features=4, n_informative=2, random_state=0, shuffle=False)\n",
    "\n",
    "import mlflow.pyfunc\n",
    "\n",
    "model_name = \"sk-learn-random-forest-reg-model\"\n",
    "model_version = 1  # replace with your model version\n",
    "\n",
    "model_uri = f\"models:/{model_name}/{model_version}\"\n",
    "model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "input_vector = X[0].reshape(1, -1)\n",
    "# Predict the output for the input vector\n",
    "output_vector = model.predict(input_vector)\n",
    "\n",
    "print(output_vector , y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.Créer un script à part qui pull le dernier modèle depuis le model registry. Plus tard vous pourrez utiliser ce script pour récupérer le modèle dans une api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 5/5 [00:02<00:00,  2.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "def get_latest_model(model_name):\n",
    "    client = MlflowClient()\n",
    "    model_version_details = client.get_latest_versions(model_name, stages=[\"Production\"])\n",
    "    if model_version_details:\n",
    "        latest_version = model_version_details[0].version\n",
    "        model_uri = f\"models:/{model_name}/{latest_version}\"\n",
    "        model = mlflow.pyfunc.load_model(model_uri)\n",
    "        return model\n",
    "    else:\n",
    "        print(f\"No production model found for {model_name}\")\n",
    "        return None\n",
    "\n",
    "# Usage:\n",
    "model = get_latest_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Entraînement d'un CNN et log des metrics dans MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir le notebook donnée par le formateur."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow_tp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
